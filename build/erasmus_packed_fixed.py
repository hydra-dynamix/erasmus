#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# Standard library imports








































































































































 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
#
-
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
A
A
C
C
C
C
C
C
C
D
D
E
E
E
E
E
E
F
F
F
G
H
I
J
K
L
L
L
M
M
M
M
M
M
O
O
P
P
P
P
P
P
P
P
P
P
R
R
R
R
S
S
S
S
T
T
T
T
V
V
W
W
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
b
b
b
b
b
b
b
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
d
d
d
d
d
d
d
d
d
d
d
d
d
d
d
d
d
d
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
h
h
h
h
h
h
h
h
h
h
h
h
h
h
h
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
j
k
k
k
k
k
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
u
v
v
v
v
v
w
w
w
w
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
y
y
y
y
y
y
y
y
y
y
y
y
y

# Third-party imports






































































 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
#
-
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
A
A
A
A
A
A
B
B
C
C
C
C
C
C
C
D
D
D
D
D
D
E
E
E
E
E
E
E
E
E
E
E
E
E
E
E
E
E
F
F
F
F
F
F
F
F
G
H
H
H
I
I
I
I
I
I
I
I
I
I
K
K
L
L
L
L
L
L
L
L
M
M
M
M
M
M
N
N
O
O
O
O
P
P
P
P
P
P
P
P
P
P
P
P
R
R
R
R
R
R
R
R
R
R
R
R
S
S
S
S
S
S
S
S
S
S
S
S
S
S
T
T
T
T
T
T
T
U
U
U
V
W
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
b
b
b
b
b
b
b
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
d
d
d
d
d
d
d
d
d
d
d
d
d
d
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
f
f
f
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
h
h
h
h
h
h
h
h
h
h
h
h
h
h
h
h
h
h
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
k
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
q
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
u
u
u
u
u
v
v
v
v
v
v
v
v
v
v
v
v
v
v
w
w
w
x
x
x
x
x
y
y
y
y
y
y
y
y
y
y
y
y
z

# Local imports






 
 
 
 
 
#
-
.
.
.
T
_
_
_
_
_
_
_
a
a
a
a
a
a
c
c
c
c
c
c
c
c
c
d
d
d
d
d
d
d
e
e
e
h
h
h
i
i
i
i
i
i
i
i
i
l
l
l
l
l
l
l
m
m
m
m
m
m
m
m
n
n
n
n
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
p
p
p
p
p
p
p
p
p
r
r
r
r
r
r
r
r
r
s
s
s
t
t
t
t
t
t
t
t
t
t
t
t
u
w
x
y



# Source: erasmus/__main__.py
"""Main entry point for the Erasmus package."""


if __name__ == "__main__":
main()

# Source: erasmus/__init__.py


# Source: erasmus/erasmus.py
"""Erasmus CLI entry point."""



logger = get_logger(__name__)


def get_setup_paths(project_root: Path = None) -> SetupPaths:
"""Get a SetupPaths instance.

Args:
project_root: Optional project root path. If not provided, uses current directory.

Returns:
SetupPaths: A SetupPaths instance
"""
return SetupPaths.with_project_root(project_root or Path.cwd())


def main():
"""Main entry point for the Erasmus CLI."""
# Initialize setup paths
setup_paths = get_setup_paths()

# Ensure directories exist
setup_paths.ensure_directories()

# Run the CLI
cli()


if __name__ == "__main__":
main()

# Source: erasmus/cli/commands.py
"""CLI interface for Erasmus.

This module provides the command-line interface for interacting with Erasmus.
It uses Click for command parsing and handling.
"""






load_dotenv()

# Configure logging
logger = get_logger(__name__)
console = Console()
task_manager = TaskManager()

SETUP_PATHS = SetupPaths.with_project_root(Path.cwd())


def log_command(f):
"""Decorator to log CLI command execution."""

@wraps(f)
def wrapper(*args, **kwargs):
cmd_name = f.__name__
with LogContext(logger, f"command_{cmd_name}"):
try:
logger.info(f"Executing command: {cmd_name}")
if kwargs:
logger.debug(f"Command arguments: {kwargs}")
result = f(*args, **kwargs)
logger.info(f"Command {cmd_name} completed successfully")
return result
except Exception:
logger.error(f"Command {cmd_name} failed", exc_info=True)
raise

return wrapper


@click.group()
@log_command
def cli():
"""Erasmus: AI Context Watcher for Development."""
return


# Add protocol commands
cli.add_command(protocol)


# === Task Management Commands ===
@cli.group()
@click.pass_context
def task(ctx):
"""Task management commands."""
return


@task.command()
@click.argument("description")
@click.pass_context
def add(ctx, description: str):
"""Add a new task with the given description."""
with LogContext(logger, "add_task"):
try:
logger.debug(f"Adding task with description: {description}")
new_task = task_manager.add_task(description)
logger.info(f"Created task {new_task.id}: {description}")
console.print(f"‚ú® Created task [bold green]{new_task.id}[/]")
console.print(f"Description: {new_task.description}")
except Exception as e:
logger.error(f"Failed to add task: {e}", exc_info=True)
console.print(f"‚ùå Failed to add task: {e}", style="red")
ctx.exit(1)


@task.command()
@click.argument("task_id")
@click.argument("status", type=click.Choice(["pending", "in_progress", "completed", "blocked"]))
@click.pass_context
def status(ctx, task_id: str, status: str):
"""Update the status of a task."""
with LogContext(logger, "update_task_status"):
try:
logger.debug(f"Updating task {task_id} status to {status}")
task_status = TaskStatus[status.lower()]
task_manager.update_task_status(task_id, task_status)
logger.info(f"Updated task {task_id} status to {status}")
console.print(f"üìù Updated task [bold]{task_id}[/] status to [bold green]{status}[/]")
except Exception as e:
logger.error(f"Failed to update task status: {e}", exc_info=True)
console.print(f"‚ùå Failed to update task status: {e}", style="red")
ctx.exit(1)


@task.command()
@click.option(
"--status",
type=click.Choice(["pending", "in_progress", "completed", "blocked"]),
help="Filter tasks by status",
)
@click.pass_context
def list(ctx, status: str | None = None):
"""List all tasks, optionally filtered by status."""
with LogContext(logger, "list_tasks"):
try:
logger.debug(f"Listing tasks with status filter: {status}")
task_status = TaskStatus[status.lower()] if status else None
tasks = task_manager.list_tasks(task_status)

if not tasks:
logger.info("No tasks found")
console.print("No tasks found.")
return

logger.info(f"Found {len(tasks)} tasks")
table = Table(show_header=True)
table.add_column("ID", style="cyan")
table.add_column("Description")
table.add_column("Status", style="green")

for task in tasks:
logger.debug(f"Adding task to table: {task.id} - {task.status}")
table.add_row(
task.id,
task.description,
task.status.value.lower().replace("_", " "),
)

console.print(table)
except Exception as e:
logger.error(f"Failed to list tasks: {e}", exc_info=True)
console.print(f"‚ùå Failed to list tasks: {e}", style="red")
ctx.exit(1)


@task.command()
@click.argument("task_id")
@click.argument("note")
@click.pass_context
def note(ctx, task_id: str, note: str):
"""Add a note to a task."""
with LogContext(logger, "add_task_note"):
try:
logger.debug(f"Adding note to task {task_id}: {note}")
task_manager.add_note_to_task(task_id, note)
logger.info(f"Added note to task {task_id}")
console.print(f"üìù Added note to task [bold]{task_id}[/]")
except Exception as e:
logger.error(f"Failed to add note: {e}", exc_info=True)
console.print(f"‚ùå Failed to add note: {e}", style="red")
ctx.exit(1)


# === Git Commands ===
@cli.group()
@click.pass_context
def git(ctx):
"""Git operations."""
return


@git.command()
@click.pass_context
def status(ctx):
"""Show git repository status."""
with LogContext(logger, "git_status"):
try:
logger.debug("Getting repository status")
git_manager = GitManager(Path.cwd())
state = git_manager.get_repository_state()
logger.info(
f"Repository status - Branch: {state['branch']}, "
+ f"Staged: {len(state['staged'])}, "
+ f"Unstaged: {len(state['unstaged'])}, "
+ f"Untracked: {len(state['untracked'])}",
)
console.print(state)
except Exception as e:
logger.error(f"Failed to get git status: {e}", exc_info=True)
console.print(f"‚ùå Failed to get git status: {e}", style="red")
ctx.exit(1)


@git.command()
@click.argument("message")
@click.pass_context
def commit(ctx, message: str):
"""Create a git commit with the given message."""
with LogContext(logger, "git_commit"):
try:
logger.debug(f"Attempting to commit with message: {message}")
git_manager = GitManager(Path.cwd())
if git_manager.commit_changes(message):
logger.info("Successfully committed changes")
console.print("‚ú® Changes committed successfully")
else:
logger.error("Failed to commit changes")
console.print("‚ùå Failed to commit changes", style="red")
ctx.exit(1)
except Exception as e:
logger.error(f"Failed to commit changes: {e}", exc_info=True)
console.print(f"‚ùå Failed to commit changes: {e}", style="red")
ctx.exit(1)


@git.command()
@click.argument("name")
@click.pass_context
def branch(ctx, name: str):
"""Create and switch to a new git branch."""
with LogContext(logger, "git_branch"):
try:
logger.debug(f"Creating and switching to branch: {name}")
git_manager = GitManager(Path.cwd())
git_manager._run_git_command(["checkout", "-b", name])
logger.info(f"Created and switched to branch: {name}")
console.print(f"‚ú® Created and switched to branch: [bold]{name}[/]")
except Exception as e:
logger.error(f"Failed to create/switch branch: {e}", exc_info=True)
console.print(f"‚ùå Failed to create branch: {name}", style="red")
ctx.exit(1)


# === Project Commands ===
@cli.command()
@click.pass_context
def setup(ctx):
"""Set up a new project with necessary files and configuration."""
with LogContext(logger, "project_setup"):
try:
logger.info("Starting project setup")

setup_env()
logger.info("Project setup completed successfully")
console.print("‚ú® Project setup complete")
except Exception as e:
logger.error(f"Project setup failed: {e}", exc_info=True)
console.print(f"‚ùå Project setup failed: {e}", style="red")
ctx.exit(1)


@cli.command()
@click.option(
"--type",
type=click.Choice(["architecture", "progress", "tasks", "context"]),
help="Type of file to update",
)
@click.option("--content", help="New content for the file")
@click.pass_context
def update(ctx, type: str, content: str):
"""Update project files."""
with LogContext(logger, "update_file"):
try:
logger.debug(f"Updating {type} file")
update_specific_file(type, content)
logger.info(f"Successfully updated {type} file")
console.print(f"‚ú® Updated {type} file")
except Exception as e:
logger.error(f"Failed to update {type} file: {e}", exc_info=True)
console.print(f"‚ùå Failed to update {type} file: {e}", style="red")
ctx.exit(1)


@cli.command()
@click.option("--force", is_flag=True, help="Force cleanup without confirmation")
@click.pass_context
def cleanup(ctx, force: bool):
"""Remove all generated files."""
with LogContext(logger, "cleanup"):
try:
result = subprocess.run(
["bash", "scripts/cleanup.sh"], check=True, stdout=PIPE, stdin=PIPE
)
console.print(result.stdout)
logger.info("Successfully cleaned up project files")
console.print("üßπ Cleanup complete")
except Exception as e:
logger.error(f"Failed to clean up project: {e}", exc_info=True)
console.print(f"‚ùå Cleanup failed: {e}", style="red")
ctx.exit(1)


@cli.command()
def watch():
"""Watch for changes in project files and update context automatically."""
with LogContext(logger, "watch"):
try:
# Initialize file watchers

factory = WatcherFactory()

# Create watchers for markdown files
markdown_watcher = factory.create_markdown_watcher(
SETUP_PATHS.markdown_files, update_specific_file
)

# Create watchers for protocol files
protocol_files = {
"agent_registry": SETUP_PATHS.protocols_dir / "agent_registry.json",
"protocols": SETUP_PATHS.protocols_dir / "stored",
}
protocol_watcher = factory.create_markdown_watcher(
protocol_files,
lambda file_type, content: handle_protocol_context(SETUP_PATHS, file_type),
)

# Create observers for each watcher
markdown_observer = factory.create_observer(
markdown_watcher, str(SETUP_PATHS.project_root)
)
protocol_observer = factory.create_observer(
protocol_watcher, str(SETUP_PATHS.protocols_dir)
)

# Start all watchers
factory.start_all()
logger.info("üëÄ Watching for file changes...")
console.print("üëÄ Watching for file changes... Press Ctrl+C to stop")

# Flag to control the main loop
running = True

# Handle shutdown gracefully
def handle_shutdown(signum, frame):
nonlocal running
logger.info("Shutting down watchers...")
console.print("\nüõë Shutting down watchers...")
factory.stop_all()
logger.info("All watchers stopped")
console.print("‚ú® All watchers stopped successfully")
running = False

# Register signal handlers
signal.signal(signal.SIGINT, handle_shutdown)
signal.signal(signal.SIGTERM, handle_shutdown)

# Keep the main thread alive
while running:
time.sleep(1)

except Exception as e:
logger.error(f"Error in watch command: {e}", exc_info=True)
console.print(f"‚ùå Error in watch command: {e}", style="red")
raise


@cli.group()
@click.pass_context
def context(ctx):
"""Context management commands."""
return


@context.command()
@click.pass_context
def store(ctx):
"""Store the current context in the context directory."""
with LogContext(logger, "store_context"):
try:
success = store_context(SETUP_PATHS)
if success:
logger.info("Context stored successfully")
console.print("‚ú® Context stored successfully")
else:
logger.error("Failed to store context")
console.print("‚ùå Failed to store context. Check logs for details.", style="red")
ctx.exit(1)
except Exception as e:
logger.error(f"Failed to store context: {e}", exc_info=True)
console.print(f"‚ùå Failed to store context: {e}", style="red")
ctx.exit(1)


@context.command()
@click.argument(
"path", type=click.Path(exists=True, dir_okay=True, file_okay=False), required=False
)
@click.pass_context
def restore(ctx, path):
"""Restore the context from the context directory.

If PATH is provided, restore from that specific directory.
Otherwise, restore from the default project_context directory.
"""
with LogContext(logger, "restore_context"):
try:
context_path = Path(path) if path else None
success = restore_context(SETUP_PATHS, context_path)
if success:
logger.info("Context restored successfully")
console.print("‚ú® Context restored successfully")
else:
logger.error("Failed to restore context")
console.print("‚ùå Failed to restore context. Check logs for details.", style="red")
ctx.exit(1)
except Exception as e:
logger.error(f"Failed to restore context: {e}", exc_info=True)
console.print(f"‚ùå Failed to restore context: {e}", style="red")
ctx.exit(1)


@context.command()
@click.pass_context
def list(ctx):
"""List all available context directories."""
with LogContext(logger, "list_context_dirs"):
try:
context_dirs = SETUP_PATHS.context_dir.iterdir()
context_dir_names = [dir_path.name for dir_path in context_dirs if dir_path.is_dir()]

if context_dir_names:
logger.info(f"Found {len(context_dir_names)} context directories")
console.print("Available context directories:")
for i, dir_name in enumerate(context_dir_names, 1):
# Try to get a readable name from the directory
console.print(f"{i}. {dir_name}")
else:
logger.info("No context directories found")
console.print("No context directories found")

console.print("‚ú® Context directories listed successfully")
except Exception as e:
logger.error(f"Failed to list context directories: {e}", exc_info=True)
console.print(f"‚ùå Failed to list context directories: {e}", style="red")
ctx.exit(1)


@context.command()
@click.pass_context
def select(ctx):
"""Select a context directory to restore."""
with LogContext(logger, "select_context"):
try:
architecture_context_dir = select_context_dir(SETUP_PATHS)

if not architecture_context_dir:
logger.info("No context directory selected")
console.print("No context directory selected", style="yellow")
return

logger.info(f"Selected context directory: {architecture_context_dir}")
console.print(f"Selected context directory: {architecture_context_dir}")

# Confirm before restoring
prompt = "Do you want to restore this context? This will overwrite current files."
if click.confirm(prompt):
success = restore_context(SETUP_PATHS, architecture_context_dir)
if success:
logger.info("Context restored successfully")
console.print("‚ú® Context restored successfully")
else:
logger.error("Failed to restore context")
console.print(
"‚ùå Failed to restore context. Check logs for details.", style="red"
)
ctx.exit(1)
else:
logger.info("Context restoration cancelled")
console.print("Context restoration cancelled")
except Exception as e:
logger.error(f"Failed to select context: {e}", exc_info=True)
console.print(f"‚ùå Failed to select context: {e}", style="red")
ctx.exit(1)


if __name__ == "__main__":
cli()

# Source: erasmus/cli/protocol.py
"""Protocol management commands for Erasmus.

This module provides Click-based commands for managing protocols in Erasmus.
"""




# Load environment variables
load_dotenv()

logger = get_logger(__name__)
console = Console()

# Initialize environment manager
env_manager = EnvironmentManager()
context_handler = ContextFileHandler(workspace_root=Path.cwd())

# Global protocol manager instance
protocol_manager = None


async def get_protocol_manager() -> ProtocolManager:
"""Get or create the protocol manager instance."""
global protocol_manager
if protocol_manager is None:
protocol_manager = ProtocolManager()
await protocol_manager.load_registry()
await protocol_manager.register_default_prompts()  # Register default prompts
return protocol_manager


@click.group()
def protocol():
"""Protocol management commands."""
pass


@protocol.command()
def list():
"""List all available protocols."""
with LogContext(logger, "list_protocols"):
asyncio.run(_list_protocols())


async def _list_protocols():
"""Async implementation of list command."""
manager = await get_protocol_manager()

protocols = manager.list_protocols()
if not protocols:
console.print("No protocols available.")
return

table = click.style("\nAvailable Protocols:", bold=True)
for protocol in protocols:
table += f"\n- {protocol.get('name', 'Unknown')}"
table += f"\n  Triggers: {', '.join(protocol.get('triggers', []))}"
table += f"\n  Produces: {', '.join(protocol.get('produces', []))}"
table += f"\n  Consumes: {', '.join(protocol.get('consumes', []))}\n"

console.print(table)


@protocol.command()
@click.argument("name")
def restore(name: str):
"""Restore a protocol by name."""
with LogContext(logger, "restore_protocol"):
asyncio.run(_restore_protocol(name))


async def _restore_protocol(name: str):
"""Async implementation of restore command."""
manager = await get_protocol_manager()

try:
# Get the protocol
protocol = manager.get_protocol(name)
if not protocol:
console.print(f"‚ùå Protocol not found: {name}", style="red")
return

# Update the context with the protocol
await update_context_with_protocol(name)
console.print(f"‚úÖ Restored protocol: {name}")
except Exception as e:
console.print(f"‚ùå Error restoring protocol: {e}", style="red")


@protocol.command()
def select():
"""List available protocols and select one to load."""
with LogContext(logger, "select_protocol"):
asyncio.run(_select_protocol())


async def _select_protocol():
"""Async implementation of select command."""
manager = await get_protocol_manager()

protocols = manager.list_protocols()
if not protocols:
console.print("No protocols available to select.")
return

console.print("\nAvailable Protocols:")
for i, protocol in enumerate(protocols, 1):
console.print(f"{i}. {protocol.get('name', 'Unknown')}")

try:
selection = click.prompt("\nSelect a protocol (number)", type=int)
if selection < 1 or selection > len(protocols):
console.print("Invalid selection.", style="red")
return

selected_protocol = protocols[selection - 1]
await update_context_with_protocol(selected_protocol.get("name"))
console.print(f"‚úÖ Selected protocol: {selected_protocol.get('name')}")
except click.Abort:
console.print("Selection cancelled.")
except Exception as e:
console.print(f"‚ùå Error selecting protocol: {e}", style="red")


@protocol.command()
@click.argument("name")
def store(name: str):
"""Store a protocol in the protocol directory."""
with LogContext(logger, "store_protocol"):
asyncio.run(_store_protocol(name))


async def _store_protocol(name: str):
"""Async implementation of store command."""
manager = await get_protocol_manager()

try:
# Get the protocol
protocol = manager.get_protocol(name)
if not protocol:
console.print(f"‚ùå Protocol not found: {name}", style="red")
return

# Store the protocol
setup_paths = SetupPaths.with_project_root(Path.cwd())
protocols_dir = setup_paths.protocols_dir / "stored"
protocols_dir.mkdir(parents=True, exist_ok=True)

protocol_file = protocols_dir / f"{name}.json"

# Convert protocol to a serializable dictionary
protocol_dict = {}
if hasattr(protocol, "model_dump"):
protocol_dict = protocol.model_dump()
else:
protocol_dict = {
"name": protocol.get("name", name),
"description": protocol.get("description", ""),
"file_path": str(protocol.get("file_path", "")),
}

# Ensure all paths are converted to strings
for key, value in protocol_dict.items():
if isinstance(value, Path):
protocol_dict[key] = str(value)

with open(protocol_file, "w") as f:
json.dump(protocol_dict, f, indent=2)

console.print(f"‚úÖ Stored protocol: {name}")
except Exception as e:
console.print(f"‚ùå Error storing protocol: {e}", style="red")


@protocol.command()
@click.argument("name")
def delete(name: str):
"""Delete a protocol from the protocol directory."""
with LogContext(logger, "delete_protocol"):
setup_paths = SetupPaths.with_project_root(Path.cwd())
protocols_dir = setup_paths.protocols_dir / "stored"
protocol_file = protocols_dir / f"{name}.json"

if not protocol_file.exists():
console.print(f"‚ùå Protocol file not found: {protocol_file}", style="red")
return

try:
os.remove(protocol_file)
console.print(f"‚úÖ Deleted protocol: {name}")
except Exception as e:
console.print(f"‚ùå Error deleting protocol: {e}", style="red")


@protocol.command()
@click.argument("name")
@click.option("--context", help="JSON string containing context for protocol execution")
def execute(name: str, context: str):
"""Execute a specific protocol."""
with LogContext(logger, "execute_protocol"):
asyncio.run(_execute_protocol(name, context))


async def _execute_protocol(name: str, context: str):
"""Async implementation of execute command."""
manager = await get_protocol_manager()

try:
context_data = json.loads(context) if context else {}

# Execute the protocol
transitions = await manager.execute_protocol(name, context_data)

# Get the protocol
protocol = manager.get_protocol(name)
if not protocol:
console.print(f"‚ùå Protocol not found: {name}", style="red")
return

console.print(f"\nProtocol Execution Result:")
console.print(f"Protocol: {name}")
console.print(f"Next Transitions: {len(transitions)}")

if transitions:
console.print("\nNext Transitions:")
for transition in transitions:
console.print(
f"- {transition.from_agent} -> {transition.to_agent} ({transition.trigger})"
)
except Exception as e:
console.print(f"‚ùå Error executing protocol: {e}", style="red")


@protocol.command()
@click.argument("name")
@click.option("--context", help="JSON string containing context for workflow execution")
def workflow(name: str, context: str):
"""Run a workflow starting from a specific protocol."""
with LogContext(logger, "run_workflow"):
asyncio.run(_run_workflow(name, context))


async def _run_workflow(name: str, context: str):
"""Async implementation of workflow command."""
manager = await get_protocol_manager()

try:
context_data = json.loads(context) if context else {}

# Run the workflow
current_protocol = name
context = context_data.copy()
results = []

while current_protocol:
# Execute the current protocol
transitions = await manager.execute_protocol(current_protocol, context)
results.append({"protocol": current_protocol, "transitions": transitions})

# Determine next protocol based on transitions
if transitions:
# For simplicity, just take the first transition
next_transition = transitions[0]
current_protocol = next_transition.to_agent
else:
current_protocol = None

console.print(f"\nWorkflow Execution Result:")
console.print(f"Starting Protocol: {name}")
console.print(f"Total Steps: {len(results)}")

console.print("\nWorkflow Steps:")
for i, step in enumerate(results, 1):
console.print(f"{i}. {step['protocol']}")
console.print(f"   Next Transitions: {len(step['transitions'])}")
except Exception as e:
console.print(f"‚ùå Error running workflow: {e}", style="red")


async def update_context_with_protocol(protocol_name: str):
"""Update the context with the selected protocol."""
manager = await get_protocol_manager()
protocol_file = manager.get_protocol_file(protocol_name)
selected_protocol = protocol_file.read_text()
protocol = manager.get_protocol(protocol_name)

if not selected_protocol:
console.print(f"‚ùå Protocol not found: {protocol_name}", style="red")
return False

try:
# Get current context
context = context_handler.read_context()

# Update only protocol-related fields
context["current_protocol"] = selected_protocol
context["protocol_triggers"] = protocol.get("triggers", [])
context["protocol_produces"] = protocol.get("produces", [])
context["protocol_consumes"] = protocol.get("consumes", [])
context["protocol_markdown"] = protocol.get("markdown", "")

# Store updated context
context_handler.update_context(context)

console.print(f"‚úÖ Updated context with protocol: {protocol_name}")
return True
except Exception as e:
console.print(f"‚ùå Error updating context: {e}", style="red")
return False


def get_ide_env_rules_path() -> Path:
"""Get the path to the IDE environment rules file."""
# Use SetupPaths to get the correct rules file path based on IDE environment
setup_paths = SetupPaths.with_project_root(Path.cwd())
return setup_paths.rules_file

# Source: erasmus/cli/__init__.py


# Source: erasmus/cli/setup.py
"""Setup command for environment configuration."""




console = Console()


def validate_base_url(value: str) -> str:
"""Validate and transform OPENAI_BASE_URL value."""
if not value:
return "https://api.openai.com/v1"

# Ensure URL starts with http:// or https://
if not value.startswith(("http://", "https://")):
value = "https://" + value

# Remove trailing slash
return value.rstrip("/")


# Validation rules for specific fields
FIELD_VALIDATORS: dict[str, Callable[[str], str]] = {
"OPENAI_BASE_URL": validate_base_url,
}


def read_env_example() -> dict[str, str]:
"""Read default values from .env.example file."""
defaults = {}
example_path = Path(".env.example")

if not example_path.exists():
raise FileNotFoundError(
".env.example not found. Please ensure it exists in the project root."
)

with example_path.open() as f:
for line in f:
line = line.strip()
if line and not line.startswith("#"):
key, value = line.split("=", 1)
defaults[key.strip()] = value.strip()

return defaults


def get_default_prompts(defaults: dict[str, str]) -> dict[str, str]:
"""Get prompts for each environment variable."""
return {
"IDE_ENV": f"Select IDE environment (C)ursor or (W)indsurf (enter 'C' or 'W') [{defaults.get('IDE_ENV', 'C')}]: ",
"OPENAI_API_KEY": f"Enter OpenAI API key [{defaults.get('OPENAI_API_KEY', '')}]: ",
"OPENAI_BASE_URL": f"Enter OpenAI base URL [{defaults.get('OPENAI_BASE_URL', 'https://api.openai.com/v1')}]: ",
"OPENAI_MODEL": f"Enter OpenAI model [{defaults.get('OPENAI_MODEL', 'gpt-4')}]: ",
}


def prompt_for_values(prompts: dict[str, str], defaults: dict[str, str]) -> dict[str, str]:
"""Prompt for environment variable values."""
values = {}
for key, prompt in prompts.items():
while True:
value = Prompt.ask(prompt, default=defaults.get(key, ""))

# Special handling for IDE_ENV
if key == "IDE_ENV":
value = value.lower()
if value.startswith(("c", "w")):
values[key] = value[0]  # Just take the first character
break
console.print(
"[red]Invalid IDE environment. Please enter 'C' for Cursor or 'W' for Windsurf[/red]"
)
continue

# Apply field-specific validation if available
if key in FIELD_VALIDATORS:
try:
value = FIELD_VALIDATORS[key](value)
except ValueError as e:
console.print(f"[red]{str(e)}[/red]")
continue

values[key] = value
break

return values


def write_env_file(values: dict[str, str]) -> None:
"""Write environment variables to .env file."""
env_path = Path(".env")

# Read existing content if file exists
existing_content = ""
if env_path.exists():
existing_content = env_path.read_text()

# Prepare new content
new_lines = []
existing_vars = set()

# Process existing content
for line in existing_content.splitlines():
if line.strip() and not line.startswith("#"):
key = line.split("=", 1)[0].strip()
existing_vars.add(key)
if key in values:
new_lines.append(f"{key}={values[key]}")
del values[key]
else:
new_lines.append(line)
else:
new_lines.append(line)

# Add remaining new variables
for key, value in values.items():
if key not in existing_vars:
new_lines.append(f"{key}={value}")

# Write to file
env_path.write_text("\n".join(new_lines))


def setup_env() -> None:
"""Set up environment variables and project structure."""
try:
# Initialize environment manager
env_manager = EnvironmentManager()

# Read defaults from .env.example
defaults = read_env_example()

# Get prompts for each environment variable
prompts = get_default_prompts(defaults)

# Prompt for values
values = prompt_for_values(prompts, defaults)

# Set environment variables
for key, value in values.items():
if key == "IDE_ENV":
# Only set if not already set or if explicitly changed
current_ide_env = env_manager.ide_env
if not current_ide_env or current_ide_env.lower() != value.lower():
env_manager.set_ide_env(value)
else:
env_manager.set_env_var(key, value)

# Setup project structure first

path_manager = PathManager()
path_manager.ensure_directories()

# Set default agent to Orchestration Agent

async def set_default_agent():
protocol_manager = await ProtocolManager.create()
await protocol_manager.activate_protocol("Orchestration Agent")

# Get the current event loop or create a new one
try:
loop = asyncio.get_event_loop()
except RuntimeError:
loop = asyncio.new_event_loop()
asyncio.set_event_loop(loop)

# Run the async function in the event loop
loop.run_until_complete(set_default_agent())

console.print("‚ú® Environment setup complete")
console.print("‚ú® Default agent set to Orchestration Agent")

except Exception as e:
console.print(f"‚ùå Environment setup failed: {e}", style="red")
raise


if __name__ == "__main__":
# Only run as a command when executed directly

cli()

# Source: erasmus/cli/main.py
"""Main CLI entry point."""



def main():
"""Main entry point for the CLI."""
# Set up logging
log_level = os.getenv("LOG_LEVEL", "INFO")
log_dir = Path.home() / ".erasmus" / "logs"
init_logging(level=log_level, log_dir=log_dir)

# Run CLI
cli(auto_envvar_prefix='ERASMUS')

if __name__ == '__main__':
main()

# Source: erasmus/git/manager.py
"""Git operations management for Erasmus."""


logger = get_logger(__name__)

class GitManager:
"""Manages Git operations for the project."""

@log_execution()
def __init__(self, repo_path: str | Path):
"""Initialize GitManager with a repository path."""
with LogContext(logger, "init"):
self.repo_path = Path(repo_path).resolve()
logger.debug(f"Initializing GitManager with path: {self.repo_path}")
if not self._is_git_repo():
logger.info(f"No git repository found at {self.repo_path}, initializing new repo")
self._init_git_repo()
else:
logger.debug(f"Found existing git repository at {self.repo_path}")

@log_execution()
def _is_git_repo(self) -> bool:
"""Check if the path is a git repository."""
with LogContext(logger, "is_git_repo"):
try:
subprocess.run(
["git", "rev-parse", "--is-inside-work-tree"],
cwd=self.repo_path,
capture_output=True,
check=True,
)
logger.debug(f"Confirmed git repository at {self.repo_path}")
return True
except subprocess.CalledProcessError:
logger.debug(f"No git repository found at {self.repo_path}")
return False

@log_execution()
def _init_git_repo(self) -> None:
"""Initialize a new git repository."""
with LogContext(logger, "init_git_repo"):
try:
logger.debug(f"Initializing new git repository at {self.repo_path}")
subprocess.run(
["git", "init"],
cwd=self.repo_path,
check=True,
)

# Configure default user
logger.debug("Configuring default git user")
subprocess.run(
["git", "config", "user.name", "Context Watcher"],
cwd=self.repo_path,
check=True,
)
subprocess.run(
["git", "config", "user.email", "context.watcher@local"],
cwd=self.repo_path,
check=True,
)
logger.info(f"Successfully initialized git repository at {self.repo_path}")
except subprocess.CalledProcessError as e:
logger.error(f"Failed to initialize git repository: {e}", exc_info=True)
raise

@log_execution()
def _run_git_command(self, command: list[str]) -> tuple[str, str]:
"""Run a git command and return stdout and stderr."""
with LogContext(logger, f"run_git_command({' '.join(command)})"):
try:
logger.debug(f"Running git command: {' '.join(command)}")
result = subprocess.run(
["git", *command],
cwd=self.repo_path,
capture_output=True,
text=True,
check=True,
)
stdout, stderr = result.stdout.strip(), result.stderr.strip()
if stdout:
logger.debug(f"Command output: {stdout}")
if stderr:
logger.warning(f"Command stderr: {stderr}")
return stdout, stderr
except subprocess.CalledProcessError as e:
logger.error(
f"Git command failed: {' '.join(command)}",
exc_info=True,
)
return "", e.stderr.strip()

@log_execution()
def stage_all_changes(self) -> bool:
"""Stage all changes in the repository."""
with LogContext(logger, "stage_all_changes"):
try:
# Get status before staging
status_before = self._run_git_command(["status", "--porcelain"])[0]
logger.debug(f"Files to stage:\n{status_before}")

# Stage changes
self._run_git_command(["add", "-A"])

# Verify staging
status_after = self._run_git_command(["status", "--porcelain"])[0]
staged_count = len([line for line in status_after.split('\n') if line.startswith('A ')])
logger.info(f"Successfully staged {staged_count} changes")
return True
except Exception:
logger.error("Failed to stage changes", exc_info=True)
return False

@log_execution()
@log_execution()
def commit_changes(self, message: str) -> bool:
"""Commit staged changes with a given message."""
with LogContext(logger, "commit_changes"):
try:
logger.debug(f"Attempting to commit with message: {message}")

# First stage any changes
if not self.stage_all_changes():
logger.warning("No changes to commit")
return False

# Commit changes
stdout, stderr = self._run_git_command(["commit", "-m", message])
if stdout:
logger.info(f"Successfully committed changes: {stdout}")
return True
logger.warning("Commit command succeeded but no output received")
return False
except Exception:
logger.error("Failed to commit changes", exc_info=True)
return False

@log_execution()
def get_repository_state(self) -> dict[str, list[str]]:
"""Get the current state of the repository."""
with LogContext(logger, "get_repository_state"):
try:
# Get current branch
branch = self.get_current_branch()
logger.debug(f"Current branch: {branch}")

# Get status
status_output, _ = self._run_git_command(["status", "--porcelain"])
status_lines = status_output.split("\n") if status_output else []
logger.debug(f"Found {len(status_lines)} status lines")

# Parse status
staged = []
unstaged = []
untracked = []

for line in status_lines:
if not line:
continue
status = line[:2]
path = line[3:].strip()

if status.startswith("??"):
untracked.append(path)
logger.debug(f"Untracked file: {path}")
elif status[0] != " ":
staged.append(path)
logger.debug(f"Staged file: {path}")
elif status[1] != " ":
unstaged.append(path)
logger.debug(f"Unstaged file: {path}")

state = {
"branch": branch,
"staged": staged,
"unstaged": unstaged,
"untracked": untracked,
}

logger.info(
f"Repository state - Branch: {branch}, " +
f"Staged: {len(staged)}, " +
f"Unstaged: {len(unstaged)}, " +
f"Untracked: {len(untracked)}",
)
return state
except Exception:
logger.error("Failed to get repository state", exc_info=True)
state = {
"branch": "unknown",
"staged": [],
"unstaged": [],
"untracked": [],
}
logger.debug("Returning empty repository state")
return state

@log_execution()
def get_current_branch(self) -> str:
"""Get the name of the current branch."""
with LogContext(logger, "get_current_branch"):
try:
branch_output, _ = self._run_git_command(["rev-parse", "--abbrev-ref", "HEAD"])
branch = branch_output.strip()
logger.debug(f"Current branch: {branch}")
return branch
except Exception:
logger.error("Failed to get current branch", exc_info=True)
return "unknown"

# Source: erasmus/git/__init__.py


# Source: erasmus/sync/file_sync.py
"""
File Synchronization Module
=========================

This module handles synchronization of project files to the rules directory,
ensuring that context files are properly copied and updated.
"""




logger = logging.getLogger(__name__)


class FileChangeHandler(FileSystemEventHandler):
"""Handles file system events for tracked files."""

def __init__(self, filename: str, callback: Callable[[str], None]):
"""Initialize the handler.

Args:
filename: Name of the file to track
callback: Function to call when file changes
"""
super().__init__()
self.filename = filename
self.callback = callback

def on_modified(self, event):
"""Handle file modification events."""
if not event.is_directory and Path(event.src_path).name == self.filename:
self.callback(self.filename)


class FileSynchronizer:
"""Manages synchronization of project files to the rules directory."""

TRACKED_FILES = {
".erasmus/.architecture.md": "architecture",
".tasks.md": "tasks",
".progress.md": "progress",
}

def __init__(self, workspace_path: Path, rules_dir: Path):
"""Initialize the FileSynchronizer.

Args:
workspace_path: Root path of the workspace
rules_dir: Path to the rules directory (.cursorrules)
"""
self.workspace_path = workspace_path
self.rules_dir = rules_dir
self._handlers: dict[str, FileChangeHandler] = {}
self._observers: dict[str, Observer] = {}
self._sync_lock = asyncio.Lock()
self._last_sync: dict[str, float] = {}
self._debounce_delay = 0.5  # seconds
self._pending_syncs: set[str] = set()
self._running = False
self._sync_task = None
self._event_loop = None

async def start(self):
"""Start the file synchronizer."""
if self._running:
return

self._running = True
self._event_loop = asyncio.get_running_loop()

# Create rules directory if it doesn't exist
self.rules_dir.mkdir(parents=True, exist_ok=True)

# Initialize rules file if it doesn't exist
rules_file = self.rules_dir / "rules.json"
if not rules_file.exists():
safe_write_file(rules_file, "{}")

# Set up handlers and observers for each tracked file
for filename in self.TRACKED_FILES:
source_file = self.workspace_path / filename
if source_file.exists():
# Create and configure the handler
handler = FileChangeHandler(
filename,
lambda f=filename: asyncio.run_coroutine_threadsafe(
self._handle_file_change(f),
self._event_loop,
),
)
self._handlers[filename] = handler

# Set up observer for this file
observer = Observer()
observer.schedule(handler, str(source_file.parent), recursive=False)
observer.daemon = True
observer.start()
self._observers[filename] = observer

# Start processing sync tasks
self._sync_task = asyncio.create_task(self._process_syncs())

async def stop(self):
"""Stop the file synchronizer."""
if not self._running:
return

self._running = False
if self._sync_task:
self._sync_task.cancel()
with contextlib.suppress(asyncio.CancelledError):
await self._sync_task

# Stop observers
for observer in self._observers.values():
observer.stop()
observer.join()

# Clean up
self._handlers.clear()
self._observers.clear()
self._event_loop = None

async def sync_all(self):
"""Synchronize all tracked files in the workspace."""
# Create a list of tracked files that exist in the workspace
files_to_sync = [
filename for filename in self.TRACKED_FILES if (self.workspace_path / filename).exists()
]

# Synchronize each existing tracked file
for filename in files_to_sync:
await self.sync_file(filename)

async def sync_file(self, filename: str):
"""Synchronize a specific file.

Args:
filename: Name of the file to synchronize

Raises:
FileNotFoundError: If the source file does not exist or is not tracked
PermissionError: If there are permission issues
RuntimeError: If there are other synchronization errors
"""
if filename not in self.TRACKED_FILES:
logger.warning(f"Attempted to sync untracked file: {filename}")
raise FileNotFoundError(f"File is not tracked: {filename}")

source_file = self.workspace_path / filename
if not source_file.exists():
# Remove from rules if file doesn't exist
rules_file = self.rules_dir / "rules.json"
async with self._sync_lock:
try:
current_rules = (
json.loads(safe_read_file(rules_file)) if rules_file.exists() else {}
)
component = self.TRACKED_FILES[filename]
if component in current_rules:
del current_rules[component]
safe_write_file(rules_file, json.dumps(current_rules, indent=2))
except Exception as e:
logger.exception(f"Error cleaning up missing file {filename}: {e}")
raise FileNotFoundError(f"Source file does not exist: {filename}")

try:
# Read source file content
content = safe_read_file(source_file)
if content is None:
raise RuntimeError(f"Unable to read file: {filename}")

# Update rules file
rules_file = self.rules_dir / "rules.json"
async with self._sync_lock:
try:
current_rules = (
json.loads(safe_read_file(rules_file)) if rules_file.exists() else {}
)
except json.JSONDecodeError:
logger.exception("Invalid JSON in rules file")
current_rules = {}

# Update the specific component
current_rules[self.TRACKED_FILES[filename]] = content

# Write updated rules
try:
safe_write_file(rules_file, json.dumps(current_rules, indent=2))
self._last_sync[filename] = datetime.now().timestamp()
logger.debug(f"Successfully synchronized {filename}")
except PermissionError as e:
raise PermissionError(f"Permission denied writing to rules file: {e}")
except Exception as e:
raise RuntimeError(f"Failed to sync file {filename}: {e}")

except (PermissionError, RuntimeError) as e:
logger.exception(f"Error synchronizing {filename}: {e}")
raise

async def _handle_file_change(self, filename: str):
"""Handle a file change event.

Args:
filename: Name of the changed file
"""
logger.debug(f"File change detected: {filename}")
self._pending_syncs.add(filename)

async def _process_syncs(self):
"""Process pending file synchronizations."""
while self._running:
try:
if self._pending_syncs:
# Get current pending syncs
to_sync = self._pending_syncs.copy()
self._pending_syncs.clear()

# Wait for debounce delay
await asyncio.sleep(self._debounce_delay)

# Sync each file
for filename in to_sync:
await self.sync_file(filename)

await asyncio.sleep(0.1)  # Brief sleep to prevent busy loop

except Exception as e:
logger.exception(f"Error processing syncs: {e}")
await asyncio.sleep(1.0)  # Longer sleep on error

async def get_sync_status(self) -> dict[str, dict]:
"""Get the last sync time for each tracked file.

Returns:
Dict mapping filenames to status information including:
- last_sync: timestamp of last sync
- exists: whether the file exists
- synced: whether the file has been synced
"""
status = {}
rules_file = self.rules_dir / "rules.json"

try:
current_rules = json.loads(safe_read_file(rules_file)) if rules_file.exists() else {}
except json.JSONDecodeError:
logger.exception("Invalid JSON in rules file")
current_rules = {}

for filename, component in self.TRACKED_FILES.items():
source_file = self.workspace_path / filename

if source_file.exists():
source_content = safe_read_file(source_file)
is_synced = (
component in current_rules and current_rules[component] == source_content
)
status[filename] = {
"last_sync": self._last_sync.get(filename) if is_synced else None,
"exists": True,
"synced": is_synced,
}
else:
status[filename] = {
"last_sync": None,
"exists": False,
"synced": False,
}
return status

async def cleanup(self):
"""Clean up files that no longer exist in workspace.

Raises:
PermissionError: If there are permission issues writing to rules file
RuntimeError: If there are other cleanup errors
"""
try:
rules_file = self.rules_dir / "rules.json"
if not rules_file.exists():
return

async with self._sync_lock:
try:
current_rules = json.loads(safe_read_file(rules_file))
except json.JSONDecodeError:
logger.exception("Invalid JSON in rules file")
return

# Check each tracked file
for filename, component in self.TRACKED_FILES.items():
source_file = self.workspace_path / filename
if not source_file.exists() and component in current_rules:
del current_rules[component]
if filename in self._last_sync:
del self._last_sync[filename]

# Write updated rules
try:
safe_write_file(rules_file, json.dumps(current_rules, indent=2))
except PermissionError as e:
raise PermissionError(f"Permission denied writing to rules file: {e}")

except Exception as e:
logger.exception(f"Error during cleanup: {e}")
raise RuntimeError(f"Failed to clean up rules: {e}")

# Source: erasmus/sync/__init__.py
"""
File synchronization package for managing project files and rules directory.
"""


__all__ = ['FileSynchronizer']

# Source: erasmus/core/dynamic_updates.py
"""
Dynamic Update System for Context Management
==========================================

This module provides functionality for handling dynamic updates to the context
management system, including change tracking, validation, and rollback support.
"""


logger = logging.getLogger(__name__)

@dataclass
class ChangeRecord:
"""Records a single change to the context system."""
timestamp: datetime
component: str
previous_value: Any
new_value: Any
source: str
metadata: dict[str, Any]

class DynamicUpdateManager:
"""
Manages dynamic updates to the context management system.

This class handles:
- Change detection and validation
- Update application with rollback support
- Change history tracking
- Update notifications
"""

def __init__(self, context_dir: Path):
"""
Initialize the dynamic update manager.

Args:
context_dir: Directory containing context files
"""
self.context_dir = Path(context_dir)
self.changes_file = self.context_dir / "changes.json"
self.changes_file.touch(exist_ok=True)
self._load_changes()

def _load_changes(self) -> None:
"""Load the change history from disk."""
try:
if self.changes_file.stat().st_size > 0:
content = self.changes_file.read_text()
raw_changes = json.loads(content)
self.changes = [
ChangeRecord(
timestamp=datetime.fromisoformat(c["timestamp"]),
component=c["component"],
previous_value=c["previous_value"],
new_value=c["new_value"],
source=c["source"],
metadata=c["metadata"],
)
for c in raw_changes
]
else:
self.changes = []
except Exception as e:
logger.exception(f"Failed to load changes: {e}")
self.changes = []

def _save_changes(self) -> None:
"""Save the change history to disk."""
try:
changes_data = [
{
"timestamp": c.timestamp.isoformat(),
"component": c.component,
"previous_value": c.previous_value,
"new_value": c.new_value,
"source": c.source,
"metadata": c.metadata,
}
for c in self.changes
]
self.changes_file.write_text(json.dumps(changes_data, indent=2))
except Exception as e:
logger.exception(f"Failed to save changes: {e}")

def detect_changes(self, component: str, new_value: Any) -> tuple[bool, dict[str, Any] | None]:
"""
Detect if there are meaningful changes to a component.

Args:
component: Name of the component being checked
new_value: New value to compare against current state

Returns:
Tuple of (has_changes: bool, diff: Optional[Dict[str, Any]])
"""
try:
# Get the most recent change for this component
current_value = None
for change in reversed(self.changes):
if change.component == component:
current_value = change.new_value
break

if current_value is None:
return True, {"type": "initial", "component": component}

# Compare values based on type
if isinstance(new_value, str | int | float | bool):
has_changed = new_value != current_value
diff = {"type": "value_change", "old": current_value, "new": new_value} if has_changed else None
elif isinstance(new_value, list | dict):
has_changed = json.dumps(new_value, sort_keys=True) != json.dumps(current_value, sort_keys=True)
diff = {"type": "structure_change", "component": component} if has_changed else None
else:
has_changed = new_value != current_value
diff = {"type": "unknown_change", "component": component} if has_changed else None

return has_changed, diff

except Exception as e:
logger.exception(f"Error detecting changes: {e}")
return False, None

def validate_update(self, component: str, new_value: Any) -> tuple[bool, str | None]:
"""
Validate a proposed update before applying it.

Args:
component: Name of the component to update
new_value: New value to validate

Returns:
Tuple of (is_valid: bool, error_message: Optional[str])
"""
try:
# Basic validation
if component.strip() == "":
return False, "Component name cannot be empty"

if new_value is None:
return False, "New value cannot be None"

# Ensure serializable
try:
json.dumps(new_value)
except (TypeError, ValueError):
return False, "New value must be JSON serializable"

# Type-specific validation
if not isinstance(new_value, str | int | float | bool | list | dict):
return False, "New value must be a basic type (str, int, float, bool) or a container (list, dict)"

# Component-specific validation
if component == "tasks":
if not isinstance(new_value, dict):
return False, "tasks must be a dictionary"
for task_id, task_data in new_value.items():
if not isinstance(task_data, dict):
return False, f"Task {task_id} data must be a dictionary"
if "description" not in task_data:
return False, f"Task {task_id} missing description"

return True, None

except Exception as e:
logger.exception(f"Error validating update: {e}")
return False, str(e)

def apply_update(self, component: str, new_value: Any, source: str, metadata: dict[str, Any] | None = None) -> bool:
"""
Apply an update to a component with rollback support.

Args:
component: Name of the component to update
new_value: New value to apply
source: Source of the update (e.g., "file_watcher", "user", "system")
metadata: Optional metadata about the update

Returns:
bool: True if update was successful, False otherwise
"""
try:
# Validate the update
is_valid, error = self.validate_update(component, new_value)
if not is_valid:
logger.error(f"Invalid update: {error}")
return False

# Detect changes
has_changes, diff = self.detect_changes(component, new_value)
if not has_changes:
logger.info(f"No changes detected for {component}")
return True

# Get current value for rollback
current_value = None
for change in reversed(self.changes):
if change.component == component:
current_value = change.new_value
break

# Create change record
change = ChangeRecord(
timestamp=datetime.now(),
component=component,
previous_value=current_value,
new_value=new_value,
source=source,
metadata=metadata or {},
)

# Apply the change
self.changes.append(change)
self._save_changes()

logger.info(f"Successfully updated {component}")
return True

except Exception as e:
logger.exception(f"Error applying update: {e}")
return False

def rollback_last_change(self, component: str) -> bool:
"""
Rollback the last change for a component.

Args:
component: Name of the component to rollback

Returns:
bool: True if rollback was successful, False otherwise
"""
try:
# Find the last two changes for this component
last_change = None
previous_change = None

for change in reversed(self.changes):
if change.component == component:
if last_change is None:
last_change = change
else:
previous_change = change
break

if last_change is None:
logger.error(f"No changes found for {component}")
return False

# Remove the last change
self.changes.remove(last_change)

# If there was a previous change, that becomes the current state
if previous_change is not None:
logger.info(f"Rolled back {component} to previous state")
else:
logger.info(f"Rolled back {component} to initial state")

self._save_changes()
return True

except Exception as e:
logger.exception(f"Error rolling back change: {e}")
return False

def get_change_history(self, component: str | None = None, limit: int = 10) -> list[ChangeRecord]:
"""
Get the change history for a component.

Args:
component: Optional component to filter by
limit: Maximum number of changes to return

Returns:
List of ChangeRecord objects
"""
if component:
filtered_changes = [c for c in self.changes if c.component == component]
else:
filtered_changes = self.changes

return filtered_changes[-limit:]

# Source: erasmus/core/rule_applicator.py
"""
Rule Applicator Module

This module provides functionality for applying rules to code and managing rule chains.
It works in conjunction with the RulesParser to validate code against defined rules.
"""



class RuleApplicationError(Exception):
"""Exception raised when rule application fails."""

class RuleChain:
"""
Represents a chain of rules to be applied in sequence.

A rule chain maintains an ordered list of rules that should be applied
to code in the specified order, regardless of rule priorities.

Attributes:
rules (List[Rule]): Ordered list of rules in the chain
"""

def __init__(self, rules: list[Rule] | None = None):
"""
Initialize a rule chain.

Args:
rules (List[Rule], optional): Initial list of rules. Defaults to None.
"""
self.rules = rules or []

def add_rule(self, rule: Rule) -> None:
"""
Add a rule to the chain.

Args:
rule (Rule): Rule to add to the chain
"""
self.rules.append(rule)

def remove_rule(self, rule_name: str) -> None:
"""
Remove a rule from the chain by name.

Args:
rule_name (str): Name of the rule to remove

Raises:
RuleApplicationError: If the rule is not found in the chain
"""
for i, rule in enumerate(self.rules):
if rule.name == rule_name:
self.rules.pop(i)
return
raise RuleApplicationError(f"Rule '{rule_name}' not found in chain")

class RuleApplicator:
"""
Manages and applies rules to code.

This class handles the creation of rule chains and the application
of rules to code in a specified order.

Attributes:
rules (List[Rule]): Available rules for application
_rule_map (Dict[str, Rule]): Mapping of rule names to Rule objects
"""

def __init__(self, rules: list[Rule]):
"""
Initialize the rule applicator.

Args:
rules (List[Rule]): List of available rules
"""
# Sort rules by priority (higher numbers first)
self.rules = sorted(rules, key=lambda x: -x.priority)
self._rule_map = {rule.name: rule for rule in rules}

def get_rule(self, rule_name: str) -> Rule:
"""
Get a rule by name.

Args:
rule_name (str): Name of the rule to retrieve

Returns:
Rule: The requested rule

Raises:
RuleApplicationError: If the rule is not found
"""
rule = self._rule_map.get(rule_name)
if not rule:
raise RuleApplicationError(f"Rule '{rule_name}' not found")
return rule

def create_chain(self, rule_names: list[str]) -> RuleChain:
"""
Create a new rule chain from a list of rule names.

Args:
rule_names (List[str]): Names of rules to include in the chain

Returns:
RuleChain: A new rule chain containing the specified rules

Raises:
RuleApplicationError: If any rule name is not found
"""
chain = RuleChain()
for name in rule_names:
try:
chain.add_rule(self.get_rule(name))
except RuleApplicationError as e:
raise RuleApplicationError(f"Failed to create chain: {e!s}")
return chain

def apply_chain(self, chain: RuleChain, code: str) -> list[ValidationError]:
"""
Apply a chain of rules to code.

Rules are applied in the order specified by the chain, not by
their individual priorities.

Args:
chain (RuleChain): Chain of rules to apply
code (str): Code to validate

Returns:
List[ValidationError]: List of validation errors found
"""
errors = []

for rule in chain.rules:
try:
# Apply each rule individually and collect errors
rule_errors = self._apply_rule(rule, code)
errors.extend(rule_errors)
except Exception as e:
# If a rule fails to apply, add it as a validation error
errors.append(ValidationError(
rule=rule,
message=f"Failed to apply rule: {e!s}",
severity="error",
))

return errors

def _apply_rule(self, rule: Rule, code: str) -> list[ValidationError]:
"""
Apply a single rule to code.

Args:
rule (Rule): Rule to apply
code (str): Code to validate

Returns:
List[ValidationError]: List of validation errors found
"""
errors = []

try:
pattern = re.compile(rule.pattern)
matches = list(pattern.finditer(code))

if rule.type.value == "code_style":
# For code style rules, we want to ensure the pattern is NOT found
# (except for require_* rules)
if matches and not rule.name.startswith("require_"):
errors.append(ValidationError(
rule=rule,
message=f"Code violates rule: {rule.description}",
severity=rule.severity,
))
# For require_* rules, we want to ensure the pattern IS found
elif not matches and rule.name.startswith("require_"):
errors.append(ValidationError(
rule=rule,
message=f"Code does not match rule pattern: {rule.description}",
severity=rule.severity,
))
elif rule.type.value == "documentation":
# For documentation rules, we want to ensure the pattern IS found
if not matches:
errors.append(ValidationError(
rule=rule,
message=f"Code does not match rule pattern: {rule.description}",
severity=rule.severity,
))
except re.error as e:
errors.append(ValidationError(
rule=rule,
message=f"Invalid rule pattern: {e!s}",
severity="error",
))

return errors

# Source: erasmus/core/ide_integration.py
"""IDE integration module."""



logger = logging.getLogger(__name__)


def start() -> None:
"""Start the IDE integration."""
setup_paths = SetupPaths.with_project_root(Path.cwd())
rules_file = setup_paths.rules_file

try:
# Try to read existing rules
content = safe_read_file(rules_file)
json.loads(content)  # Validate JSON
except (FileNotFoundError, json.JSONDecodeError) as e:
# Create empty rules file if it doesn't exist or is invalid
logging.warning(f"Rules file error: {e}. Creating new rules file.")
safe_write_file(rules_file, "{}")
except Exception as e:
logging.exception(f"Unexpected error handling rules file: {e}")
raise

# Source: erasmus/core/rules.py
"""Rules management module."""



logger = logging.getLogger(__name__)


class RuleValidationError(Exception):
"""Exception raised for rule validation errors."""

pass


class Rule:
"""Represents a single rule."""

def __init__(self, category: str, description: str):
self.category = category
self.description = description


class RulesManager:
"""Manages project and global rules."""

def __init__(self, workspace_root: Path):
"""Initialize the rules manager.

Args:
workspace_root: Path to the project workspace root
"""
self.workspace_root = Path(workspace_root)
self.setup_paths = SetupPaths.with_project_root(workspace_root)
self.active_rules: set[Rule] = set()
self._seen_rules: set[str] = set()  # Track rules by their unique key
self._conflicting_patterns = [
(r"use\s+spaces\s+for\s+indentation", r"use\s+tabs\s+for\s+indentation"),
# Add more conflicting patterns as needed
]

def _rule_key(self, rule: Rule) -> str:
"""Generate a unique key for a rule."""
return f"{rule.category.lower()}:{rule.description.lower()}"

def add_rule(self, rule: Rule) -> bool:
"""Add a rule if it doesn't conflict with existing rules."""
key = self._rule_key(rule)
if key in self._seen_rules:
return False

# Check for conflicts
for pattern1, pattern2 in self._conflicting_patterns:
if pattern1 in rule.description.lower() and any(
pattern2 in r.description.lower() for r in self.active_rules
):
return False
if pattern2 in rule.description.lower() and any(
pattern1 in r.description.lower() for r in self.active_rules
):
return False

self.active_rules.add(rule)
self._seen_rules.add(key)
return True

def remove_rule(self, rule: Rule) -> bool:
"""Remove a rule if it exists."""
key = self._rule_key(rule)
if key in self._seen_rules:
self.active_rules.remove(rule)
self._seen_rules.remove(key)
return True
return False

def get_rules(self) -> Set[Rule]:
"""Get all active rules."""
return self.active_rules.copy()

def save_rules(self) -> None:
"""Save rules to the appropriate files."""
try:
# Save to project rules file
rules_data = {
"rules": [
{"category": r.category, "description": r.description}
for r in self.active_rules
]
}
with open(self.setup_paths.rules_file, "w") as f:
json.dump(rules_data, f, indent=2)
logger.info(f"Saved rules to {self.setup_paths.rules_file}")

# Save to global rules file
with open(self.setup_paths.global_rules_file, "w") as f:
json.dump(rules_data, f, indent=2)
logger.info(f"Saved rules to {self.setup_paths.global_rules_file}")

except Exception as e:
logger.exception(f"Failed to save rules: {e}")
raise

def load_rules(self) -> None:
"""Load rules from the appropriate files."""
try:
# Try to load from project rules file first
if self.setup_paths.rules_file.exists():
with open(self.setup_paths.rules_file) as f:
data = json.load(f)
self._load_rules_from_data(data)
return

# Fall back to global rules file
if self.setup_paths.global_rules_file.exists():
with open(self.setup_paths.global_rules_file) as f:
data = json.load(f)
self._load_rules_from_data(data)
return

logger.warning("No rules files found")
except Exception as e:
logger.exception(f"Failed to load rules: {e}")
raise

def _load_rules_from_data(self, data: Dict[str, Any]) -> None:
"""Load rules from a data dictionary."""
self.active_rules.clear()
self._seen_rules.clear()

for rule_data in data.get("rules", []):
rule = Rule(category=rule_data["category"], description=rule_data["description"])
self.add_rule(rule)

def parse_rules(self, rules_path: Path, is_global: bool = False) -> list[Rule]:
"""Parse rules from a markdown file."""
if not rules_path.exists():
return []

content = rules_path.read_text()
lines = content.strip().split("\n")

if not lines or not lines[0].startswith("# "):
raise RuleValidationError("Rules file must start with a title (# Rules)")

rules: list[Rule] = []
current_category = ""
current_subcategory = None
seen_rules = set()  # Track rules within this file

for line in lines[1:]:
line = line.strip()
if not line:
continue

if line.startswith("## "):
# New category
current_category = line[3:].strip()
current_subcategory = None
if "/" in current_category:
current_category, current_subcategory = current_category.split("/", 1)
continue

if line.startswith("- "):
# New rule
description = line[2:].strip()
# Remove any trailing comments
if "#" in description:
description = description.split("#")[0].strip()

rule = Rule(description, current_category, current_subcategory, is_global)
rule_key = self._rule_key(rule)

# Check for duplicates
if rule_key in seen_rules:
raise RuleValidationError(f"Duplicate rule: {description}")

seen_rules.add(rule_key)
rules.append(rule)

# Check for conflicts
self._check_conflicting_rules(rules)

return rules

def validate_code(self, code: str, rules: list[Rule] | None = None) -> list[str]:
"""Validate code against a set of rules.

Args:
code: The code to validate
rules: Optional list of rules to validate against. If None, uses active_rules.

Returns:
List of validation errors, empty if code is valid
"""
if rules is None:
rules = list(self.active_rules)

errors = []
tree = ast.parse(code)

for rule in rules:
if rule.category.lower() == "code style":
if "type hints" in rule.description.lower():
# Check for type hints
for node in ast.walk(tree):
if isinstance(node, ast.FunctionDef):
if not node.returns or not any(
isinstance(arg.annotation, ast.Name) for arg in node.args.args
):
errors.append(f"Missing type hints in function {node.name}")

elif rule.category.lower() == "documentation":
if "document all functions" in rule.description.lower():
# Check for docstrings
for node in ast.walk(tree):
if isinstance(node, ast.FunctionDef) and not ast.get_docstring(node):
errors.append(f"Missing docstring in function {node.name}")

elif rule.category.lower() == "code quality":
if "no print statements" in rule.description.lower():
# Check for print statements
for node in ast.walk(tree):
if (
isinstance(node, ast.Call)
and isinstance(node.func, ast.Name)
and node.func.id == "print"
):
errors.append("Print statement found")

return errors

def export_rules(self, project_rules_path: Path, global_rules_path: Path) -> None:
"""Export current rules to files.

Args:
project_rules_path: Path to export project rules
global_rules_path: Path to export global rules
"""
# Separate rules by type
project_rules = [r for r in self.active_rules if not r.is_global]
global_rules = [r for r in self.active_rules if r.is_global]

# Export project rules
self._write_rules_file(project_rules, project_rules_path, "Project Rules")

# Export global rules
self._write_rules_file(global_rules, global_rules_path, "Global Rules")

def import_rules(self, project_rules_path: Path, global_rules_path: Path) -> None:
"""Import rules from files.

Args:
project_rules_path: Path to import project rules from
global_rules_path: Path to import global rules from
"""
# Clear existing rules
self.active_rules.clear()
self._seen_rules.clear()

# Import project rules
if project_rules_path.exists():
project_rules = self.parse_rules(project_rules_path)
for rule in project_rules:
key = self._rule_key(rule)
self.active_rules.add(rule)
self._seen_rules.add(key)

# Import global rules
if global_rules_path.exists():
global_rules = self.parse_rules(global_rules_path, is_global=True)
for rule in global_rules:
key = self._rule_key(rule)
if key not in self._seen_rules:
self.active_rules.add(rule)
self._seen_rules.add(key)

def _write_rules_file(self, rules: list[Rule], path: Path, title: str) -> None:
"""Write rules to a file.

Args:
rules: List of rules to write
path: Path to write rules to
title: Title for the rules file
"""
# Read existing content if file exists
existing_content = ""
if path.exists():
with path.open("r") as f:
existing_content = f.read().strip()

# If existing content is a complete JSON object, preserve it
if existing_content.startswith("{") and existing_content.endswith("}"):
return

# Group rules by category
rules_by_category: dict[str, list[Rule]] = {}
for rule in rules:
category = rule.category
if category not in rules_by_category:
rules_by_category[category] = []
rules_by_category[category].append(rule)

# Write rules to file
with path.open("w") as f:
# Write title if not already present
if not existing_content.startswith(f"# {title}"):
f.write(f"# {title}\n\n")

for category in sorted(rules_by_category.keys()):
f.write(f"## {category}\n")
for rule in sorted(rules_by_category[category], key=lambda r: r.description):
f.write(f"- {rule.description}\n")
f.write("\n")

def get_rules_for_file(self, file_path: Path) -> list[Rule]:
"""Get applicable rules for a file.

Args:
file_path: Path to the file to get rules for

Returns:
List of applicable rules
"""
# For now, return all active rules
# In the future, this could be enhanced to filter rules based on file type, path, etc.
return list(self.active_rules)

# Source: erasmus/core/rules_parser.py
"""
Rules Parser Module

This module provides functionality for parsing and validating rules in the context management system.
It includes classes for representing rules and their types, as well as a parser for reading rules from files.
"""




class RuleType(Enum):
"""Enumeration of rule types."""
CODE_STYLE = "code_style"
DOCUMENTATION = "documentation"
SECURITY = "security"
PERFORMANCE = "performance"
TESTING = "testing"

@dataclass
class Rule:
"""
Represents a single rule with its properties.

Attributes:
name (str): Unique identifier for the rule
description (str): Human-readable description of the rule
type (RuleType): Type of the rule
pattern (str): Regular expression pattern to match
severity (str): Severity level (error, warning, info)
priority (int): Rule priority (higher numbers = higher priority)
order (int): Original order in the rules file
"""
name: str
description: str
type: RuleType
pattern: str
severity: str
priority: int = 0
order: int = 0

def __post_init__(self):
"""Validate rule properties after initialization."""
if not self.name:
raise ValueError("Rule name cannot be empty")
if not self.description:
raise ValueError("Rule description cannot be empty")
if not self.pattern:
raise ValueError("Rule pattern cannot be empty")
if self.severity not in ["error", "warning", "info"]:
raise ValueError("Invalid severity level")

class RuleValidationError(Exception):
"""Exception raised when rule validation fails."""

@dataclass
class ValidationError:
"""
Represents a validation error found during rule checking.

Attributes:
rule (Rule): The rule that was violated
message (str): Description of the violation
line_number (Optional[int]): Line number where the violation occurred
severity (str): Severity level of the violation
"""
rule: Rule
message: str
line_number: int | None = None
severity: str = "error"

class RulesParser:
"""
Parser for reading and validating rules from files.

This class handles reading rules from markdown files, parsing their content,
and validating code against the rules.

Attributes:
rules_file (Path): Path to the rules file
rules (List[Rule]): List of parsed rules
_cached_rules (Optional[List[Rule]]): Cached rules for performance
"""

def __init__(self, rules_file: Path):
"""
Initialize the RulesParser with a rules file.

Args:
rules_file (Path): Path to the rules file to parse

Raises:
RuleValidationError: If the rules file is invalid
"""
self.rules_file = Path(rules_file)
self._cached_rules: list[Rule] | None = None
self.rules = self._parse_rules()

def _parse_rules(self) -> list[Rule]:
"""
Parse rules from the rules file.

Returns:
List[Rule]: List of parsed rules

Raises:
RuleValidationError: If the rules file is invalid
"""
if not self.rules_file.exists():
raise RuleValidationError(f"Rules file not found: {self.rules_file}")

try:
content = self.rules_file.read_text()
if not content.strip():
return []

rules = []
current_rule: dict[str, Any] = {}
order = 0

for line in content.splitlines():
line = line.strip()
if not line or line.startswith("#"):
continue

if line.startswith("rule:"):
if current_rule:
current_rule["order"] = order
rules.append(self._create_rule(current_rule))
order += 1
current_rule = {}
current_rule["name"] = line.split(":", 1)[1].strip()
elif ":" in line:
key, value = line.split(":", 1)
current_rule[key.strip()] = value.strip()

if current_rule:
current_rule["order"] = order
rules.append(self._create_rule(current_rule))

if not rules:
raise RuleValidationError("No valid rules found in file")

# Sort rules by priority (higher priority first), then by original order
rules.sort(key=lambda x: (-x.priority, x.order))
return rules

except (ValueError, KeyError) as e:
raise RuleValidationError(f"Error parsing rules file: {e!s}")
except Exception as e:
raise RuleValidationError(f"Unexpected error parsing rules file: {e!s}")

def _create_rule(self, rule_data: dict[str, Any]) -> Rule:
"""
Create a Rule object from parsed data.

Args:
rule_data (Dict[str, Any]): Dictionary of rule properties

Returns:
Rule: Created Rule object

Raises:
RuleValidationError: If required fields are missing or invalid
"""
required_fields = ["name", "description", "type", "pattern", "severity"]
missing_fields = [field for field in required_fields if field not in rule_data]

if missing_fields:
raise RuleValidationError(f"Missing required fields: {', '.join(missing_fields)}")

try:
rule_type = RuleType(rule_data["type"])
except ValueError:
raise RuleValidationError(f"Invalid rule type: {rule_data['type']}")

try:
priority = int(rule_data.get("priority", 0))
except ValueError:
raise RuleValidationError("Priority must be an integer")

return Rule(
name=rule_data["name"],
description=rule_data["description"],
type=rule_type,
pattern=rule_data["pattern"],
severity=rule_data["severity"],
priority=priority,
order=rule_data.get("order", 0),
)

def validate_code(self, code: str) -> list[ValidationError]:
"""
Validate code against all rules.

Args:
code (str): Code to validate

Returns:
List[ValidationError]: List of validation errors found
"""
errors = []

for rule in self.rules:
try:
pattern = re.compile(rule.pattern)
matches = list(pattern.finditer(code))

if rule.type == RuleType.CODE_STYLE:
# For code style rules, we want to ensure the pattern is NOT found
if matches and rule.name == "no_print_statements":
errors.append(ValidationError(
rule=rule,
message=f"Code violates rule: {rule.description}",
severity=rule.severity,
))
# For other code style rules, we want to ensure the pattern IS found
elif not matches and rule.name != "no_print_statements":
errors.append(ValidationError(
rule=rule,
message=f"Code does not match rule pattern: {rule.description}",
severity=rule.severity,
))
elif rule.type == RuleType.DOCUMENTATION:
# For documentation rules, we want to ensure the pattern IS found
if not matches:
errors.append(ValidationError(
rule=rule,
message=f"Code does not match rule pattern: {rule.description}",
severity=rule.severity,
))
except re.error as e:
errors.append(ValidationError(
rule=rule,
message=f"Invalid rule pattern: {e!s}",
severity="error",
))

return errors

def reload_rules(self) -> None:
"""Force reload of rules from file."""
self._cached_rules = None
self.rules = self._parse_rules()

def parse_rules_file(self, file_path: Path) -> list[Rule]:
"""Parse rules from a markdown file.

Args:
file_path: Path to the markdown file containing rules

Returns:
List of Rule objects parsed from the file

Raises:
FileNotFoundError: If the rules file does not exist
RuleParsingError: If there are errors parsing the rules
"""
try:
content = safe_read_file(file_path)
return self.parse_rules_content(content)
except FileNotFoundError:
logging.exception(f"Rules file not found: {file_path}")
raise
except Exception as e:
logging.exception(f"Error parsing rules from {file_path}: {e}")
raise RuleParsingError(f"Failed to parse rules from {file_path}: {e}")

# Source: erasmus/core/__init__.py


# Source: erasmus/core/task.py
"""
Task Management System
====================

This module provides classes for managing development tasks and their lifecycle.
It includes functionality for task creation, status updates, and serialization.

Classes:
TaskStatus: Constants for task states
Task: Represents a single development task
TaskManager: Manages a collection of tasks
"""



class TaskStatus:
"""
Task status constants.

This class defines constants representing different states of a task
throughout its lifecycle. Used by the Task class to track current status.
"""
PENDING = "pending"       # Task is acknowledged but not started
IN_progress = "in_progress"  # Task is actively being worked on
COMPLETED = "completed"   # Task has been finished
BLOCKED = "blocked"       # Task is blocked by another task or external factor
NOT_STARTED = "not_started"  # Task has been created but not scheduled

class Task:
"""
Represents a single development task with tracking information.

This class provides methods for managing a task's state, including
serialization/deserialization and metadata tracking. Each task has a
unique ID, description, status, and timestamps for lifecycle events.

Attributes:
id (str): Unique identifier for the task
description (str): Detailed description of the task
status (str): Current status from TaskStatus constants
created_at (float): Unix timestamp when task was created
updated_at (float): Unix timestamp when task was last updated
completion_time (Optional[float]): Unix timestamp when task was completed
notes (List[str]): List of additional notes or comments for the task
"""
def __init__(self, id: str, description: str):
"""
Initialize a new Task with given ID and description.

Args:
id (str): Unique identifier for the task
description (str): Detailed description of what the task involves
"""
self.id = id
self.description = description
self.status = TaskStatus.NOT_STARTED
self.created_at = time.time()
self.updated_at = time.time()
self.completion_time = None
self.notes = []

def to_dict(self) -> dict:
"""
Convert task to dictionary representation for serialization.

Returns:
dict: Dictionary containing all task attributes
"""
return {
"id": self.id,
"description": self.description,
"status": self.status,
"created_at": self.created_at,
"updated_at": self.updated_at,
"completion_time": self.completion_time,
"notes": self.notes,
}

@classmethod
def from_dict(cls, data: dict) -> 'Task':
"""
Create a Task instance from a dictionary representation.

Args:
data (dict): Dictionary containing task attributes

Returns:
Task: New Task instance with restored attributes
"""
task = cls(data["id"], data["description"])
task.status = data["status"]
task.created_at = data["created_at"]
task.updated_at = data["updated_at"]
task.completion_time = data["completion_time"]
task.notes = data["notes"]
return task

class TaskManager:
"""
Manages a collection of tasks and provides operations for the task lifecycle.

This class handles creating, retrieving, updating, and listing tasks. It also
provides serialization/deserialization to integrate with the context tracking
system.

Attributes:
tasks (Dict[str, Task]): Dictionary mapping task IDs to Task objects
"""
def __init__(self, tasks: dict[str, dict] | None = None):
"""
Initialize a new TaskManager with optional initial tasks.

Args:
tasks (Optional[Dict[str, dict]]): Dictionary of tasks to initialize with. Can be
either Task objects or dictionaries to deserialize.
"""
self.tasks: dict[str, Task] = {}
if tasks:
self.tasks = {
task_id: Task.from_dict(task_data) if isinstance(task_data, dict) else task_data
for task_id, task_data in tasks.items()
}

def add_task(self, description: str) -> Task:
"""
Add a new task with the given description.

Creates a new Task with an automatically assigned sequential ID and
adds it to the task collection.

Args:
description (str): Description of the new task

Returns:
Task: The newly created Task object
"""
task_id = str(len(self.tasks) + 1)
task = Task(task_id, description)
self.tasks[task_id] = task
return task

def get_task(self, task_id: str) -> Task | None:
"""
Retrieve a task by its ID.

Args:
task_id (str): ID of the task to retrieve

Returns:
Optional[Task]: The Task if found, None otherwise
"""
return self.tasks.get(task_id)

def list_tasks(self, status: str | None = None) -> list[Task]:
"""
List all tasks, optionally filtered by status.

Args:
status (Optional[str]): If provided, only tasks with this status
will be returned

Returns:
List[Task]: List of tasks matching the filter criteria
"""
tasks = list(self.tasks.values())
if status:
tasks = [t for t in tasks if t.status == status]
return tasks

def update_task_status(self, task_id: str, status: str) -> None:
"""
Update a task's status.

Args:
task_id (str): ID of the task to update
status (str): New status to set
"""
if task := self.get_task(task_id):
task.status = status
task.updated_at = time.time()
if status == TaskStatus.COMPLETED:
task.completion_time = time.time()

def add_note_to_task(self, task_id: str, note: str) -> None:
"""
Add a note to a task.

Args:
task_id (str): ID of the task to add the note to
note (str): Content of the note to add
"""
if task := self.get_task(task_id):
task.notes.append(note)
task.updated_at = time.time()

@classmethod
def from_dict(cls, data: dict[str, dict]) -> 'TaskManager':
"""
Create a TaskManager from a dictionary representation.

Args:
data (Dict[str, dict]): Dictionary mapping task IDs to task data dictionaries

Returns:
TaskManager: New TaskManager instance with restored tasks
"""
manager = cls()
if isinstance(data, dict):
manager.tasks = {
task_id: Task.from_dict(task_data)
for task_id, task_data in data.items()
}
return manager

# Source: erasmus/core/context.py
"""
Context Management System
======================

This module provides classes for managing context files and rules
in the Erasmus project.
"""



# Configure logging
logger = get_logger(__name__)


class ContextValidationError(Exception):
"""Raised when context file validation fails."""


class ContextFileHandler:
"""Handles reading, writing, and validation of context files."""

def __init__(self, workspace_root: str | Path):
"""Initialize the context file handler.

Args:
workspace_root: Path to the workspace root directory
"""
self.setup_paths = SetupPaths.with_project_root(workspace_root)
self.workspace_root = Path(workspace_root)
self.context_dir = self.workspace_root / ".erasmus" / "context"
self.rules_file = self.setup_paths.rules_file
self.global_rules_file = self.setup_paths.global_rules_file
self.context_file = self.setup_paths.rules_file

# Create context directory if it doesn't exist
self.context_dir.mkdir(exist_ok=True)

def _parse_markdown_rules(self, content: str) -> dict[str, list[str] | dict[str, list[str]]]:
"""Parse markdown content into a rules dictionary.

Args:
content: Markdown content to parse

Returns:
Dict containing parsed rules
"""
rules: dict[str, list[str] | dict[str, list[str]]] = {}
current_section = None
current_subsection = None

try:
lines = content.split("\n")
for line in lines:
line = line.strip()
if not line:
continue

# Title (#)
if line.startswith("# "):
continue

# Main section (##)
if line.startswith("## "):
current_section = line[3:].strip()
current_subsection = None
rules[current_section] = []

# Subsection (###)
elif line.startswith("### "):
if current_section is not None:
current_subsection = line[4:].strip()
if isinstance(rules[current_section], list):
rules[current_section] = {}
rules[current_section][current_subsection] = []  # type: ignore

# List item
elif line.startswith("- "):
if current_section is not None:
item = line[2:].strip()
if current_subsection is None:
if isinstance(rules[current_section], list):
rules[current_section].append(item)  # type: ignore
else:
if isinstance(rules[current_section], dict):
rules[current_section][current_subsection].append(item)  # type: ignore

return rules

except Exception as e:
logger.error(f"Failed to parse rules: {e}")
return {}

def read_rules(self) -> dict[str, list[str] | dict[str, list[str]]]:
"""Read and parse the project rules file.

Returns:
Dict containing parsed rules
"""
try:
content = self.rules_file.read_text()
return self._parse_markdown_rules(content)
except FileNotFoundError:
return {}
except Exception as e:
logger.error(f"Failed to read rules file: {e}")
return {}

def read_global_rules(self) -> dict[str, list[str] | dict[str, list[str]]]:
"""Read and parse the global rules file.

Returns:
Dict containing parsed global rules
"""
try:
content = self.global_rules_file.read_text()
return self._parse_markdown_rules(content)
except FileNotFoundError:
return {}
except Exception as e:
logger.error(f"Failed to read global rules file: {e}")
return {}

def read_context(self) -> dict[str, Any]:
"""Read and parse the context file.

Returns:
Dict containing context configuration
"""
try:
if not self.context_file.exists():
return {
"project_root": str(self.workspace_root),
"active_rules": [],
"global_rules": [],
"file_patterns": ["*.py", "*.md"],
"excluded_paths": ["venv/", "__pycache__/"],
}

content = self.context_file.read_text()
context = json.loads(content)
return context

except json.JSONDecodeError:
# If JSON is invalid, return default context
return {
"project_root": str(self.workspace_root),
"active_rules": [],
"global_rules": [],
"file_patterns": ["*.py", "*.md"],
"excluded_paths": ["venv/", "__pycache__/"],
}
except Exception as e:
logger.error(f"Failed to read context file: {e}")
return {
"project_root": str(self.workspace_root),
"active_rules": [],
"global_rules": [],
"file_patterns": ["*.py", "*.md"],
"excluded_paths": ["venv/", "__pycache__/"],
}

def update_context(self, new_context: dict[str, Any], partial: bool = False) -> None:
"""Update the context file.

Args:
new_context: New context configuration
partial: If True, only update specified fields
"""
try:
if partial:
current_context = self.read_context()
current_context.update(new_context)
new_context = current_context

# Write the updated context
self.context_file.write_text(json.dumps(new_context, indent=2))
except Exception as e:
logger.error(f"Failed to update context: {e}")

# Source: erasmus/core/watcher.py
"""
File Watching System
==================

This module provides classes for monitoring file changes in the project.
It includes specialized watchers for different file types and use cases.

Classes:
BaseWatcher: Generic file system event handler
MarkdownWatcher: Specialized watcher for markdown documentation files
ScriptWatcher: Specialized watcher for monitoring script files
WatcherFactory: Factory class for creating and managing watchers
"""




# Configure logging and console
logger = get_logger(__name__)
console = Console()

class BaseWatcher(FileSystemEventHandler):
"""Base class for file system event handlers."""

def __init__(self, file_paths: dict[str, Path], callback: Callable[[str, str], None]):
"""Initialize the watcher.

Args:
file_paths: Dictionary mapping file keys to their paths
callback: Function to call when a file changes
"""
super().__init__()
# Store both the original mapping and the normalized paths
self.file_paths = file_paths
self.callback = callback  # Store the callback
self._path_mapping = {}
for key, path in file_paths.items():
resolved = str(path.resolve())
self._path_mapping[resolved] = key
logger.debug(f"Watching {key}: {resolved}")
self._event_lock = Lock()
self._last_events: dict[str, float] = {}

def _should_process_event(self, event_path: str) -> bool:
"""Check if an event should be processed based on debouncing.

Args:
event_path: Path of the file that triggered the event

Returns:
True if the event should be processed, False otherwise
"""
with self._event_lock:
current_time = time.time()
last_time = self._last_events.get(event_path, 0)

# Debounce events within 0.1 seconds
if current_time - last_time < 0.1:
return False

self._last_events[event_path] = current_time
return True

def _get_file_key(self, file_path: str) -> str | None:
"""Get the file key for a given path.

Args:
file_path: Path to look up

Returns:
File key if found, None otherwise
"""
try:
resolved_path = str(Path(file_path).resolve())
logger.debug(f"Looking up file key for: {resolved_path}")
logger.debug(f"Known paths: {list(self._path_mapping.keys())}")
return self._path_mapping.get(resolved_path)
except Exception as e:
logger.exception(f"Error getting file key: {e}")
return None

@log_execution()
def _handle_event(self, event: FileSystemEvent) -> None:
"""Handle a file system event.

Args:
event: The file system event to handle
"""
if event.is_directory:
logger.debug(f"Ignoring directory event: {event.src_path}")
return

file_path = event.src_path
if not self._should_process_event(file_path):
logger.debug(f"Debouncing event for: {file_path}")
return

file_key = self._get_file_key(file_path)
if file_key is None:
logger.debug(f"No file key found for: {file_path}")
return

with LogContext(logger, f"handle_event({file_key})"):
try:
if os.path.exists(file_path):
with open(file_path) as f:
content = f.read()
# Always accept the content since validation is disabled
logger.info(f"üìù Detected changes in {file_key}")
self.callback(file_key, content)
else:
# For deletion events, we still want to notify with empty content
logger.info(f"File deleted: {file_key}")
self.callback(file_key, "")
except Exception:
logger.error(
f"Error handling event for {file_path}",
exc_info=True,
)

def _validate_content(self, content: str) -> bool:
"""Validate file content.

Args:
content: Content to validate

Returns:
Always returns True to accept any content
"""
return True

def on_modified(self, event: FileSystemEvent) -> None:
"""Handle file modification events."""
self._handle_event(event)

def on_created(self, event: FileSystemEvent) -> None:
"""Handle file creation events."""
self._handle_event(event)

def on_deleted(self, event: FileSystemEvent) -> None:
"""Handle file deletion events."""
self._handle_event(event)

def on_moved(self, event: FileSystemEvent) -> None:
"""Handle file movement events."""
self._handle_event(event)

class MarkdownWatcher(BaseWatcher):
"""Specialized watcher for markdown documentation files."""
def __init__(self, file_paths: dict[str, Path], callback: Callable[[str], None]):
super().__init__(file_paths, callback)

def _validate_content(self, content: str) -> bool:
"""Validate markdown content.

Args:
content: Content to validate

Returns:
True if content is valid markdown, False otherwise
"""
# Basic markdown validation
lines = content.split("\n")
if not lines:
return False

# Check for title
return lines[0].startswith("# ")

class ScriptWatcher(BaseWatcher):
"""Specialized watcher for script files.

TODO:
- Integrate LSP for real-time validation
- Add linting checks on file changes
- Add dynamic unit test runner
- Add context section for focus=path/to/script.py to track active development
"""
def __init__(self, file_paths: dict[str, Path | str], callback: Callable[[str], None]):
"""Initialize the script watcher.

Args:
file_paths: Dictionary mapping script keys to paths
callback: Function to call when scripts change
"""
# Validate all paths
normalized_paths = {}
for key, path in file_paths.items():
path = Path(path)
if not str(path).endswith('.py'):
raise ValueError(f"Script path {path} must end with .py")
normalized_paths[key] = path

super().__init__(normalized_paths, callback)

def _validate_content(self, content: str) -> bool:
"""Validate Python script content.

Args:
content: Content to validate

Returns:
True if content is valid Python, False otherwise
"""
try:
ast.parse(content)
return True
except SyntaxError:
return False

def run_observer(observer: Observer):
"""Run an observer in a separate thread.

Args:
observer: Observer to run
"""
observer.start()
try:
while observer.is_alive():
observer.join(1)
except KeyboardInterrupt:
observer.stop()
observer.join()

def create_file_watchers(setup_files: dict[str, Path],
update_callback: Callable[[str], None],
script_path: Path,
restart_callback: Callable[[str], None]) -> tuple[Observer, Observer]:
"""Create and configure file watchers.

Args:
setup_files: Dictionary mapping file keys to their paths
update_callback: Callback for file updates
script_path: Path to the script file
restart_callback: Callback for script restarts

Returns:
Tuple of (markdown_observer, script_observer)
"""
# Create watchers
markdown_watcher = MarkdownWatcher(setup_files, update_callback)
script_watcher = ScriptWatcher(script_path, restart_callback)

# Create observers
markdown_observer = Observer()
script_observer = Observer()

# Schedule watchers
markdown_observer.schedule(markdown_watcher, str(script_path.parent), recursive=False)
script_observer.schedule(script_watcher, str(script_path.parent), recursive=False)

return markdown_observer, script_observer

class WatcherFactory:
"""Factory class for creating and managing watchers."""

def __init__(self):
"""Initialize the factory."""
self.observers: list[Observer] = []

def create_markdown_watcher(self, file_paths: dict[str, Path], callback: Callable[[str], None]) -> MarkdownWatcher:
"""Create a markdown watcher.

Args:
file_paths: Dictionary mapping file keys to their paths
callback: Function to call when files change

Returns:
Configured MarkdownWatcher
"""
return MarkdownWatcher(file_paths, callback)

def create_script_watcher(self, file_paths: dict[str, Path], callback: Callable[[str], None]) -> ScriptWatcher:
"""Create a script watcher.

Args:
file_paths: Dictionary mapping script keys to paths
callback: Function to call when scripts change

Returns:
Configured ScriptWatcher
"""
return ScriptWatcher(file_paths, callback)

def create_observer(self, watcher: FileSystemEventHandler, directory: str) -> Observer:
"""Create and configure an observer.

Args:
watcher: Event handler to use
directory: Directory to watch

Returns:
Configured Observer
"""
observer = Observer()
observer.schedule(watcher, directory, recursive=False)
self.observers.append(observer)
return observer

def start_all(self) -> None:
"""Start all observers."""
for observer in self.observers:
if not observer.is_alive():
observer.start()

def stop_all(self) -> None:
"""Stop all observers."""
for observer in self.observers:
observer.stop()
observer.join()
self.observers.clear()

# Source: erasmus/ide/sync_integration.py
"""
Synchronization Integration Module
===============================

This module provides integration between the FileSynchronizer and CursorContextManager,
handling bi-directional synchronization of context files.
"""



logger = logging.getLogger(__name__)


class SyncIntegration:
"""Handles integration between FileSynchronizer and CursorContextManager."""

def __init__(self, context_manager, workspace_path: Path):
"""Initialize the sync integration."""
self.context_manager = context_manager
self.workspace_path = workspace_path
self._running = False
self._sync_lock = asyncio.Lock()
self._file_change_events = {}
self._sync_tasks = []
self._last_sync = {}
self._update_retries = {}
self.source_files = {
"architecture": workspace_path / ".erasmus/.architecture.md",
"progress": workspace_path / ".progress.md",
"tasks": workspace_path / ".tasks.md",
}

async def start(self):
"""Start the synchronization integration."""
self._running = True
# Perform initial sync
await self.sync_all()

async def stop(self):
"""Stop the sync integration."""
self._running = False
# Cancel any pending sync tasks
for task in self._sync_tasks:
if not task.done():
task.cancel()
self._sync_tasks.clear()
self._file_change_events.clear()

async def sync_all(self):
"""Synchronize all components."""
try:
# Clear any existing events
self._file_change_events.clear()

# Process each component sequentially to avoid race conditions
for component in ["architecture", "progress", "tasks"]:
file_path = self.source_files[component]
if file_path.exists():
try:
# Read content
content = file_path.read_text()

# Create event for this sync
self._file_change_events[component] = asyncio.Event()
update_successful = False

# Try update with retries
for attempt in range(2):
if attempt > 0:
logger.info(
f"Retrying initial sync for {component} (attempt {attempt + 1})"
)
await asyncio.sleep(0.2)

update_task = asyncio.create_task(
self.context_manager.queue_update(component, content),
)
self._sync_tasks.append(update_task)

try:
# Wait for update with timeout
success = await asyncio.wait_for(update_task, timeout=5.0)

if success:
# Verify the update
rules_content = safe_read_file(self.context_manager.rules_file)
if rules_content:
rules = json.loads(rules_content)
if component in rules and rules[component] == content:
update_successful = True
break

if attempt == 0:
logger.warning(
f"Initial sync verification failed for {component}, will retry"
)
else:
logger.error(
f"Initial sync verification failed for {component} after retry"
)

except asyncio.TimeoutError:
logger.exception(f"Timeout during initial sync of {component}")
if attempt == 1:  # Second attempt
raise
finally:
if update_task in self._sync_tasks:
self._sync_tasks.remove(update_task)

# Set event based on final result
if update_successful:
self._file_change_events[component].set()

# Wait briefly between components
await asyncio.sleep(0.2)

finally:
# Clean up the event
if component in self._file_change_events:
del self._file_change_events[component]

except Exception as e:
logger.exception(f"Error during sync_all: {e}")
raise

async def handle_file_change(self, file_path: Path) -> None:
"""Handle changes to source files."""
if not self._running:
return

try:
# Get the component key from the file name
component = file_path.stem.lower()
if component not in ["architecture", "progress", "tasks"]:
return

# Create event before any processing
self._file_change_events[component] = asyncio.Event()
update_successful = False

try:
# Acquire lock to prevent concurrent updates to the same file
async with self._sync_lock:
# Read the updated content
content = file_path.read_text()

# Update rules file directly
rules_file = self.context_manager.rules_file
try:
current_rules = (
json.loads(safe_read_file(rules_file)) if rules_file.exists() else {}
)
except json.JSONDecodeError:
current_rules = {}

# Update the specific component
current_rules[component] = content

# Write updated rules
safe_write_file(rules_file, json.dumps(current_rules, indent=2))
self._last_sync[component] = content
update_successful = True

# Set event based on final result
if update_successful:
self._file_change_events[component].set()

finally:
# Clean up the event after all retries
if component in self._file_change_events:
# Wait briefly to ensure any waiters have processed
await asyncio.sleep(0.1)
del self._file_change_events[component]

except Exception as e:
logger.exception(f"Error handling file change for {file_path}: {e}")
raise

async def handle_context_change(self, component: str, content: str) -> None:
"""Handle changes to context."""
if not self._running or component not in self.source_files:
return

try:
async with self._sync_lock:
file_path = self.source_files[component]
if not file_path.exists():
return

current_content = safe_read_file(file_path)

# Only update if content is different and no file change event is pending
if content != current_content and component not in self._file_change_events:
# Write atomically using temporary file
temp_path = file_path.with_suffix(".tmp")
try:
safe_write_file(temp_path, content)
temp_path.replace(file_path)
finally:
if temp_path.exists():
temp_path.unlink()

except Exception as e:
logger.exception(f"Error handling context change for {component}: {e}")
raise

# Source: erasmus/ide/cursor_integration.py
"""
cursor IDE Integration Module
==========================

This module provides specialized integration with the cursor IDE,
handling context synchronization and rule formatting specific to
cursor's requirements.
"""





logger = logging.getLogger(__name__)


class CursorContextManager:
"""Manages context updates for the cursor IDE."""

def __init__(self, workspace_path: Path):
"""Initialize the CursorContextManager."""
self.workspace_path = workspace_path

# Use SetupPaths to get the correct rules file path based on IDE environment
setup_paths = SetupPaths.with_project_root(workspace_path)
rules_dir = setup_paths.rules_file
self.rules_file = rules_dir / "rules.json"

self.batch_delay = 0.1  # seconds
self._update_queue = asyncio.Queue()
self._current_rules = {}
self._update_task = None
self._running = False
self._error_counts = defaultdict(int)
self._lock = asyncio.Lock()
self._watcher = None
self._processing = False
self._last_write_time = 0
self._write_event = asyncio.Event()
self._update_complete = asyncio.Event()
self._pending_updates = {}
self._external_changes = {}
self._sync_integration = None
self._observer = None
self._recovery_task = None
self._update_events = {}
self._file_change_queue = asyncio.Queue()
self._file_change_task = None
self._thread_queue = asyncio.Queue()  # Queue for thread-safe communication
self._thread_task = None

async def start(self):
"""Start the context manager."""
if self._running:
return

try:
# Create rules directory if it doesn't exist
rules_dir = self.rules_file.parent
rules_dir.mkdir(parents=True, exist_ok=True)

# Initialize rules file if it doesn't exist
try:
if not self.rules_file.exists():
# Write empty JSON object to file
safe_write_file(self.rules_file, "{}")

# Load current rules
content = safe_read_file(self.rules_file)
self._current_rules = json.loads(content) if content else {}
except (json.JSONDecodeError, FileNotFoundError) as e:
logger.exception(f"Error loading rules file: {e}")
self._current_rules = {}
# Ensure we have a valid rules file
safe_write_file(self.rules_file, "{}")

# Set running flag before starting tasks
self._running = True

# Start the update processing loop
self._update_task = asyncio.create_task(self._process_updates())

# Start the file change processing loop
self._file_change_task = asyncio.create_task(self._process_file_changes())

# Start the thread queue processing loop
self._thread_task = asyncio.create_task(self._process_thread_queue())

# Initialize file watcher
self._watcher = CursorRulesHandler(self)
self._observer = Observer()
self._observer.schedule(self._watcher, str(self.workspace_path), recursive=False)
self._observer.daemon = True
self._observer.start()

# Start recovery task
self._recovery_task = asyncio.create_task(self._monitor_and_recover())

# Initialize sync integration
self._sync_integration = SyncIntegration(self, self.workspace_path)

# Wait for tasks to be ready
await asyncio.sleep(0.2)

# Start sync integration and perform initial sync
await self._sync_integration.start()

# Wait for initial sync to complete
await asyncio.sleep(0.5)

except Exception as e:
logger.exception(f"Error starting context manager: {e}")
# Clean up on error
await self.stop()
raise

async def stop(self):
"""Stop the context manager."""
if not self._running:
return

self._running = False

# Stop the observer
if self._observer:
self._observer.stop()
self._observer.join()
self._observer = None

# Stop sync integration
if self._sync_integration:
await self._sync_integration.stop()

# Cancel tasks
for task in [
self._update_task,
self._recovery_task,
self._file_change_task,
self._thread_task,
]:
if task and not task.done():
task.cancel()
with contextlib.suppress(asyncio.CancelledError):
await task

# Clear state
self._watcher = None
self._current_rules = {}
self._pending_updates.clear()
self._external_changes.clear()
self._update_task = None
self._recovery_task = None
self._update_events.clear()
self._file_change_task = None
self._thread_task = None

async def queue_update(self, component: str, content: Any) -> bool:
"""Queue an update for processing."""
if not self._running:
logger.warning("CursorContextManager is not running")
return False

try:
# Create an event for this update
update_event = asyncio.Event()
self._update_events[component] = update_event

# Queue the update
self._pending_updates[component] = content
await self._update_queue.put((component, content))

try:
# Wait for update to be processed
await asyncio.wait_for(update_event.wait(), timeout=5.0)

# Wait briefly for file system sync
await asyncio.sleep(0.1)

# Verify the update was written correctly
verify_content = safe_read_file(self.rules_file)
verify_rules = json.loads(verify_content) if verify_content else {}

if component in verify_rules and verify_rules[component] == content:
return True

logger.error(f"Update verification failed for {component}")
return False

except asyncio.TimeoutError:
logger.exception(f"Timeout waiting for update to complete: {component}")
return False

except Exception as e:
logger.exception(f"Error queueing update: {e}")
return False

finally:
# Clean up
if component in self._update_events:
del self._update_events[component]
if component in self._pending_updates:
del self._pending_updates[component]

async def _process_updates(self):
"""Process queued updates."""
while self._running:
try:
# Get the next update
component, content = await self._update_queue.get()

# Process update immediately
async with self._lock:
try:
# Read current rules
rules_content = safe_read_file(self.rules_file)
current = json.loads(rules_content) if rules_content else {}

# Store external changes that aren't being updated
new_external_changes = {}
for comp, cont in self._external_changes.items():
if comp not in self._pending_updates:
current[comp] = cont
else:
new_external_changes[comp] = cont
self._external_changes = new_external_changes

# Apply update
current[component] = content

# Write update atomically
temp_file = self.rules_file.with_suffix(".tmp")
try:
safe_write_file(temp_file, json.dumps(current, indent=2))
temp_file.replace(self.rules_file)

# Update current rules
self._current_rules = current.copy()

# Notify waiters
if component in self._update_events:
self._update_events[component].set()
if component in self._pending_updates:
del self._pending_updates[component]

# Notify sync integration of context change
if self._sync_integration:
try:
await self._sync_integration.handle_context_change(
component, content
)
except Exception as e:
logger.exception(f"Error in sync integration: {e}")

finally:
if temp_file.exists():
temp_file.unlink()

except Exception as e:
logger.exception(f"Error processing update for {component}: {e}")
# Set event even on error to prevent timeouts
if component in self._update_events:
self._update_events[component].set()
if component in self._pending_updates:
del self._pending_updates[component]

except Exception as e:
logger.exception(f"Error in update loop: {e}")
await asyncio.sleep(0.1)

async def _process_file_changes(self):
"""Process file change events."""
while self._running:
try:
# Get the next file change
file_path = await self._file_change_queue.get()

# Process the file change
if self._sync_integration:
await self._sync_integration.handle_file_change(file_path)

except Exception as e:
logger.exception(f"Error processing file change: {e}")
await asyncio.sleep(0.1)

async def _process_thread_queue(self):
"""Process items from the thread-safe queue."""
while self._running:
try:
# Get the next item from the thread queue
file_path = await self._thread_queue.get()

# Queue it for file change processing
await self._file_change_queue.put(file_path)

except Exception as e:
logger.exception(f"Error processing thread queue: {e}")
await asyncio.sleep(0.1)

async def _monitor_and_recover(self):
"""Monitor the rules file and recover from errors."""
while self._running:
try:
# Check if rules file is valid
content = safe_read_file(self.rules_file)
try:
json.loads(content) if content else {}
except json.JSONDecodeError:
logger.warning("Rules file corrupted, restoring from current state")

# Brief wait before next check
await asyncio.sleep(1.0)

except Exception as e:
logger.exception(f"Error in recovery monitor: {e}")
await asyncio.sleep(1.0)


class CursorRulesHandler(FileSystemEventHandler):
"""Handles file system events for the rules file."""

def __init__(self, manager: CursorContextManager):
"""Initialize the handler."""
self.manager = manager
self._last_event_time = {}
self._debounce_delay = 0.1  # 100ms debounce

def on_modified(self, event: FileSystemEvent):
"""Handle file modification events."""
if not event.is_directory:
file_path = Path(event.src_path)
current_time = time.time()

# Debounce events for the same file
if file_path in self._last_event_time:
if current_time - self._last_event_time[file_path] < self._debounce_delay:
return

self._last_event_time[file_path] = current_time

# Handle rules file changes
if file_path == self.manager.rules_file:
try:
# Read the current rules
content = safe_read_file(self.manager.rules_file)
if not content:
return

current = json.loads(content)

# Store external changes
for comp, cont in current.items():
if comp not in self.manager._pending_updates:
self.manager._external_changes[comp] = cont

except Exception as e:
logger.exception(f"Error handling external change: {e}")

# Handle source file changes
elif file_path.suffix == ".md" and file_path.stem.lower() in [
"architecture",
"progress",
"tasks",
]:
try:
# Put the file path directly into the thread-safe queue
self.manager._thread_queue.put_nowait(file_path)
except Exception as e:
logger.exception(f"Error queueing file change: {e}")

# Source: erasmus/ide/__init__.py
"""
IDE Integration Package
===================

This package provides integration with various IDEs, handling context
synchronization and IDE-specific requirements.
"""


__all__ = ['CursorContextManager']

# Source: erasmus/utils/logging.py
"""Centralized logging configuration for Erasmus.

This module provides a consistent logging setup across the application with:
    - Configurable log levels
- Contextual logging with thread and process info
- Log rotation
- Performance tracking
- Debug logging with rich formatting
"""



# Configure console for rich output
console = Console()


class LogContext:
"""Context manager for tracking operation timing and context."""

def __init__(self, logger: logging.Logger, operation: str):
self.logger = logger
self.operation = operation
self.start_time = None

def __enter__(self):
self.start_time = time.time()
self.logger.debug(f"Starting {self.operation}")
return self

def __exit__(self, exc_type, exc_val, exc_tb):
duration = time.time() - self.start_time
if exc_type:
self.logger.error(
f"{self.operation} failed after {duration:.2f}s",
exc_info=(exc_type, exc_val, exc_tb),
)
else:
self.logger.debug(f"Completed {self.operation} in {duration:.2f}s")


def get_logger(
name: str,
level: str | int = "INFO",
log_file: Path | None = None,
max_bytes: int = 10 * 1024 * 1024,  # 10MB
backup_count: int = 5,
) -> logging.Logger:
"""Get a configured logger instance.

Args:
name: Logger name, typically __name__
level: Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
log_file: Optional file to log to
max_bytes: Maximum size of each log file
backup_count: Number of backup files to keep

Returns:
Configured logger instance
"""
logger = logging.getLogger(name)

# Convert string level to int if needed
if isinstance(level, str):
level = getattr(logging, level.upper())
logger.setLevel(level)

# Remove existing handlers
logger.handlers.clear()

# Create formatters
file_formatter = logging.Formatter(
"%(asctime)s [%(process)d:%(thread)d] %(name)s - %(levelname)s - %(message)s",
)
console_formatter = logging.Formatter(
"%(message)s",  # Rich handler adds its own formatting
)

# Add console handler with rich formatting
console_handler = RichHandler(
rich_tracebacks=True,
show_time=True,
show_path=True,
)
console_handler.setFormatter(console_formatter)
logger.addHandler(console_handler)

# Add file handler if specified
if log_file:
file_handler = logging.handlers.RotatingFileHandler(
log_file,
maxBytes=max_bytes,
backupCount=backup_count,
)
file_handler.setFormatter(file_formatter)
logger.addHandler(file_handler)

return logger


def log_execution(level: str = "DEBUG") -> Callable:
"""Decorator to log function execution with timing.

Args:
level: Log level for the timing message

Returns:
Decorator function
"""

def decorator(func: Callable) -> Callable:
@wraps(func)
def wrapper(*args: Any, **kwargs: Any) -> Any:
logger = logging.getLogger(func.__module__)
log_level = getattr(logging, level.upper())

start_time = time.time()
try:
result = func(*args, **kwargs)
duration = time.time() - start_time
logger.log(
log_level,
f"{func.__name__} completed in {duration:.2f}s",
)
return result
except Exception as e:
duration = time.time() - start_time
logger.error(
f"{func.__name__} failed after {duration:.2f}s: {e!s}",
exc_info=True,
)
raise

return wrapper

return decorator


def init_logging(
level: str | int = "INFO",
log_dir: Path | None = None,
) -> None:
"""Initialize global logging configuration.

Args:
level: Default log level
log_dir: Directory for log files
"""
if log_dir:
log_dir.mkdir(parents=True, exist_ok=True)
log_file = log_dir / "erasmus.log"
else:
log_file = None

# Configure root logger
root_logger = get_logger(
"erasmus",
level=level,
log_file=log_file,
)

# Log startup information
root_logger.info("Initializing Erasmus logging system")
root_logger.debug(f"Python version: {sys.version}")
root_logger.debug(f"Log level: {logging.getLevelName(root_logger.level)}")
if log_file:
root_logger.debug(f"Log file: {log_file}")

# Source: erasmus/utils/file.py
"""
File Utility Functions
==================

This module provides utility functions for safe file operations.
"""



def safe_read_file(file_path: str | Path) -> str:
"""
Safely read a file's contents.

Args:
file_path: Path to the file to read

Returns:
str: File contents, or empty string if file doesn't exist
"""
try:
path = Path(file_path)
if path.exists():
return path.read_text()
return ""
except Exception as e:
print(f"Error reading file {file_path}: {e}")
return ""


def safe_write_file(file_path: str | Path, content: str) -> bool:
"""
Safely write content.

Args:
file_path: Path to the file to write
content: Content to write to the file

Returns:
bool: True if write was successful, False otherwise
"""
try:
path = Path(file_path)

# Create parent directories if they don't exist
path.parent.mkdir(parents=True, exist_ok=True)

# Write the new content
path.write_text(content)
return True

except Exception as e:
print(f"Error writing file {file_path}: {e}")
return False


def ensure_file_exists(file_path: str | Path, content: str | None = None) -> bool:
"""
Ensure a file exists, optionally creating it with content.

Args:
file_path: Path to the file
content: Optional content to write if file doesn't exist

Returns:
bool: True if file exists or was created, False otherwise
"""
try:
path = Path(file_path)

# Create parent directories if they don't exist
path.parent.mkdir(parents=True, exist_ok=True)

if not path.exists():
path.write_text(content or "")
return True

except Exception as e:
print(f"Error ensuring file exists {file_path}: {e}")
return False

# Source: erasmus/utils/path_constants.py
"""Constants and configuration for path management."""


# File names
RULES_FILES: Final[Dict[str, str]] = {
"windsurf": ".windsurfrules",
"cursor": ".cursorrules",
}

# Directory names
DIRECTORIES: Final[Dict[str, str]] = {
"config": ".erasmus",
"cache": ".erasmus/cache",
"logs": ".erasmus/logs",
"protocols": ".erasmus/protocols",
"stored_protocols": ".erasmus/protocols/stored",
"stored_context": ".erasmus/context",
}

# Markdown files
MARKDOWN_FILES: Final[Dict[str, str]] = {
"architecture": ".architecture.md",
"progress": ".progress.md",
"tasks": ".tasks.md",
}

# Script files
SCRIPT_FILES: Final[Dict[str, str]] = {
"erasmus": "erasmus.py",
}

# Global rules paths
GLOBAL_RULES_PATHS: Final[Dict[str, Path]] = {
"windsurf": Path.home() / ".codeium" / "windsurf" / "memories" / "global_rules.md",
"cursor": Path.cwd() / "global_rules.md",
}

# IDE markers
IDE_MARKERS: Final[Dict[str, list[Path]]] = {
"windsurf": [
Path.home() / ".codeium" / "windsurf",
],
"cursor": [
Path.home() / ".cursor",
],
}

# Default IDE environment
DEFAULT_IDE_ENV: Final[str] = "cursor"

# Source: erasmus/utils/env_manager.py
"""Environment variable management."""


console = Console()


class EnvironmentManager:
"""Singleton class for managing environment variables."""

_instance: Optional["EnvironmentManager"] = None
_ide_env: Optional[str] = None
_env_vars: Dict[str, str] = {}
_api_key: Optional[str] = None
_base_url: Optional[str] = None
_model: Optional[str] = None

def __new__(cls) -> "EnvironmentManager":
if cls._instance is None:
cls._instance = super().__new__(cls)
cls._instance._load_environment()
return cls._instance

def _load_environment(self) -> None:
"""Load environment variables from .env file."""
# Load .env file if it exists
env_path = Path(".env")
if env_path.exists():
load_dotenv(env_path)

# Set IDE environment
self._ide_env = os.getenv("IDE_ENV", "").lower()
if not self._ide_env:
self._ide_env = DEFAULT_IDE_ENV

# Load all environment variables
self._env_vars = {key: os.getenv(key, "") for key in os.environ}

@property
def ide_env(self) -> str:
"""Get the current IDE environment."""
return self._ide_env

def set_ide_env(self, value: str) -> None:
"""Set the IDE environment and update .env file."""
value = value.lower()
if not (value.startswith("c") or value.startswith("w")):
raise ValueError("IDE_ENV must start with 'C' for cursor or 'W' for windsurf")

# Map to full IDE name
if value.startswith("w"):
value = "windsurf"
elif value.startswith("c"):
value = "cursor"

# Update environment variable
os.environ["IDE_ENV"] = value
self._ide_env = value
self._env_vars["IDE_ENV"] = value

# Update .env file
self._update_env_file()

def set_env_var(self, key: str, value: str) -> None:
"""Set an environment variable and update .env file."""
# Update environment variable
os.environ[key] = value
self._env_vars[key] = value

# Update .env file
self._update_env_file()

def get_env_var(self, key: str, default: str = "") -> str:
"""Get an environment variable value."""
return self._env_vars.get(key, default)

def _update_env_file(self) -> None:
"""Update the .env file with all environment variables."""
env_path = Path(".env")

# Read existing content if file exists
existing_content = {}
if env_path.exists():
with open(env_path, "r") as f:
for line in f:
line = line.strip()
if line and not line.startswith("#"):
key, value = line.split("=", 1)
existing_content[key.strip()] = value.strip()

# Update with current environment variables
for key, value in self._env_vars.items():
existing_content[key] = value

# Write updated content
content = []
for key, value in existing_content.items():
content.append(f"{key}={value}")

# Write to file
env_path.write_text("\n".join(content) + "\n")

def prompt_for_ide_env(self) -> str:
"""Prompt the user for the IDE environment."""
while not self.is_windsurf or not self.is_cursor:
user_input = Prompt.ask("Enter the IDE environment (windsurf/cursor)")
if user_input.lower().startswith("w"):
self._ide_env = "windsurf"
if user_input.lower().startswith("c"):
self._ide_env = "cursor"
console.print("[red]Invalid IDE environment. Please enter 'windsurf' or 'cursor'[/red]")
return self._ide_env

@property
def is_cursor(self) -> bool:
"""Check if the current IDE is Cursor."""
return self._ide_env.startswith("c")

@property
def is_windsurf(self) -> bool:
"""Check if the current IDE is Windsurf."""
return self._ide_env.startswith("w")

def prompt_for_openai_credentials(self) -> None:
"""Prompt the user for OpenAI credentials."""
self._api_key = Prompt.ask("Enter your OpenAI API key")
self._base_url = Prompt.ask("Enter your OpenAI base URL")
self._model = Prompt.ask("Enter your OpenAI model")

def get_openai_credentials():
"""Get OpenAI credentials from environment variables."""
api_key = os.environ.get("OPENAI_API_KEY", "sk-1234")
base_url = os.environ.get("OPENAI_BASE_URL", "https://api.openai.com/v1")
model = os.environ.get("OPENAI_MODEL", "gpt-4o")
return api_key, base_url, model

# Source: erasmus/utils/paths.py
"""Path management for Erasmus project files."""




logger = logging.getLogger(__name__)


class PathManager:
"""Centralized path management for the Erasmus project."""

def __init__(self, project_root: Optional[Path] = None):
"""Initialize the path manager.

Args:
project_root: Optional project root path. If not provided, uses current directory.
"""
self.project_root = project_root or Path.cwd()
self._ide_env = self._detect_ide_environment()
self._env_manager = EnvironmentManager()

# Initialize paths
self._erasmus_dir = self.project_root / ".erasmus"
self._config_dir = self._erasmus_dir / "config"
self._cache_dir = self._erasmus_dir / "cache"
self._logs_dir = self._erasmus_dir / "logs"
self._context_dir = self._erasmus_dir / "context"
self._protocols_dir = self._erasmus_dir / "protocols"
self._stored_protocols_dir = self._protocols_dir / "stored"
self._registry_file = self._protocols_dir / "registry.json"
self._env_file = self.project_root / ".env"

# Create directories
for directory in [self._config_dir, self._cache_dir, self._logs_dir, self._context_dir]:
directory.mkdir(parents=True, exist_ok=True)

def ensure_directories(self) -> None:
"""Ensure all required directories exist."""
try:
# Create .erasmus directory
self._erasmus_dir.mkdir(exist_ok=True)

# Create all subdirectories
for dir_name, dir_path in DIRECTORIES.items():
if dir_name != "config":  # Skip .erasmus itself
full_path = self.project_root / dir_path
full_path.mkdir(parents=True, exist_ok=True)
logger.debug(f"Ensured directory exists: {full_path}")

logger.info("Successfully created all required directories")
except Exception as e:
logger.error(f"Error creating directories: {e}")
raise

def prompt_for_ide_env(self) -> str:
"""Prompt the user for the IDE environment."""

while True:
user_input = click.prompt("Enter the IDE environment (windsurf/cursor)")
if user_input.lower().startswith("w"):
ide_env = "windsurf"
os.environ["IDE_ENV"] = ide_env
return ide_env
if user_input.lower().startswith("c"):
ide_env = "cursor"
os.environ["IDE_ENV"] = ide_env
return ide_env
print("Invalid IDE environment. Please enter 'windsurf' or 'cursor'")

def _detect_ide_environment(self) -> str:
"""Detect the current IDE environment.

Returns:
str: The detected IDE environment ('windsurf' or 'cursor')
"""
# Check environment variable first
ide_env = os.getenv("IDE_ENV", "").lower()
if ide_env:
if ide_env == "windsurf" or ide_env.startswith("w"):
return "windsurf"
if ide_env == "cursor" or ide_env.startswith("c"):
return "cursor"

# Try to detect based on current working directory or known IDE paths
cwd = self.project_root
is_windsurf = False
is_cursor = False

# windsurf-specific detection
windsurf_markers = [
Path.home() / ".codeium" / "windsurf",
cwd / ".windsurfrules",
]

# cursor-specific detection
cursor_markers = [
cwd / ".cursorrules",
Path.home() / ".cursor",
]

# Check windsurf markers
for marker in windsurf_markers:
if marker.exists():
is_windsurf = True

# Check cursor markers
for marker in cursor_markers:
if marker.exists():
is_cursor = True

if is_windsurf and is_cursor:
# If both are detected, prefer windsurf
return "windsurf"

# Default to cursor
return "cursor"

@property
def ide_env(self) -> str:
"""Get the current IDE environment."""
return self._ide_env

@property
def rules_file(self) -> Path:
"""Get the rules file path based on IDE environment."""

load_dotenv()
ide_env = os.getenv("IDE_ENV", "").lower()

# Map to the correct key for RULES_FILES dictionary
if ide_env == "windsurf" or ide_env.startswith("w"):
ide_key = "windsurf"
elif ide_env == "cursor" or ide_env.startswith("c"):
ide_key = "cursor"
else:
ide_key = DEFAULT_IDE_ENV  # Default to cursor

return Path.cwd() / RULES_FILES[ide_key]

@property
def global_rules_file(self) -> Path:
"""Get the global rules file path based on IDE environment."""
return self._erasmus_dir / GLOBAL_RULES_PATHS[self._ide_env]

@property
def erasmus_dir(self) -> Path:
"""Get the erasmus directory path."""
return self._erasmus_dir

@property
def config_dir(self) -> Path:
"""Get the configuration directory path."""
return self._config_dir

@property
def cache_dir(self) -> Path:
"""Get the cache directory path."""
return self._cache_dir

@property
def logs_dir(self) -> Path:
"""Get the logs directory path."""
return self._logs_dir

@property
def context_dir(self) -> Path:
"""Get the context directory path."""
return self._context_dir

@property
def protocols_dir(self) -> Path:
"""Get the protocols directory path."""
return self._protocols_dir

@property
def stored_protocols_dir(self) -> Path:
"""Get the stored protocols directory path."""
return self._stored_protocols_dir

@property
def registry_file(self) -> Path:
"""Get the registry file path."""
return self._registry_file

@property
def env_file(self) -> Path:
"""Get the environment file path."""
return self._env_file

@property
def stored_context(self) -> Path:
"""Get the stored context directory path."""
return self._context_dir

@property
def markdown_files(self) -> Dict[str, Path]:
"""Get all markdown file paths."""
return {
"architecture": self._erasmus_dir / MARKDOWN_FILES["architecture"],
"progress": self._erasmus_dir / MARKDOWN_FILES["progress"],
"tasks": self._erasmus_dir / MARKDOWN_FILES["tasks"],
}

def get_protocol_file(self, protocol_name: str) -> Path:
"""Get the path to a protocol file.

Args:
protocol_name: Name of the protocol

Returns:
Path: Path to the protocol file
"""
return self._stored_protocols_dir / f"{protocol_name}.md"

def get_protocol_json(self, protocol_name: str) -> Path:
"""Get the path to a protocol JSON file.

Args:
protocol_name: Name of the protocol

Returns:
Path: Path to the protocol JSON file
"""
return self._stored_protocols_dir / f"{protocol_name}.json"

def model_dump(self) -> Dict[str, Any]:
"""Get a dictionary representation of all paths."""
return {
"project_root": self.project_root,
"ide_env": self.ide_env,
"rules_file": self.rules_file,
"global_rules_file": self.global_rules_file,
"protocols_dir": self.protocols_dir,
"stored_protocols_dir": self.stored_protocols_dir,
"registry_file": self.registry_file,
"markdown_files": self.markdown_files,
"config_dir": self.config_dir,
"cache_dir": self.cache_dir,
"logs_dir": self.logs_dir,
"env_file": self.env_file,
"stored_context": self.stored_context,
}

def create_context_files(self) -> None:
"""Create the context directory and files if they don't exist."""
try:
# Create context directory
context_dir = self.context_dir
context_dir.mkdir(parents=True, exist_ok=True)

# Create timestamp-based context directory
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
context_subdir = context_dir / f"context_{timestamp}"
context_subdir.mkdir(parents=True, exist_ok=True)

# Create the context files
for file_name in [".architecture.md", ".progress.md", ".tasks.md"]:
file_path = context_subdir / file_name
if not file_path.exists():
file_path.touch()

logger.debug("Created context files in %s", context_subdir)
except Exception as e:
logger.error("Failed to create context files: %s", e)
raise


class FilePaths(BaseModel):
"""Base class for file paths."""

architecture: Path = Field(default_factory=lambda: Path(MARKDOWN_FILES["architecture"]))
progress: Path = Field(default_factory=lambda: Path(MARKDOWN_FILES["progress"]))
tasks: Path = Field(default_factory=lambda: Path(MARKDOWN_FILES["tasks"]))


class SetupPaths(BaseModel):
"""Paths for project setup and configuration."""

project_root: Path
erasmus_dir: Path
config_dir: Path
cache_dir: Path
logs_dir: Path
rules_file: Path
global_rules_file: Path
architecture_file: Path
progress_file: Path
tasks_file: Path
context_dir: Path
protocols_dir: Path
stored_protocols_dir: Path
registry_file: Path
markdown_files: Dict[str, Path]
script_files: Dict[str, Path]
env_file: Path
_initialized: bool = False
_env_manager: Optional[EnvironmentManager] = None

@classmethod
def with_project_root(cls, project_root: Path) -> "SetupPaths":
"""Create a SetupPaths instance with the given project root.

Args:
project_root: Path to the project root directory

Returns:
SetupPaths instance
"""
# Initialize environment manager

env_manager = EnvironmentManager()

# Create instance
instance = cls(
project_root=project_root,
erasmus_dir=project_root / ".erasmus",
config_dir=project_root / ".erasmus" / "config",
cache_dir=project_root / ".erasmus" / "cache",
logs_dir=project_root / ".erasmus" / "logs",
rules_file=project_root / RULES_FILES[env_manager.ide_env],
global_rules_file=project_root / GLOBAL_RULES_PATHS[env_manager.ide_env],
architecture_file=project_root / MARKDOWN_FILES["architecture"],
progress_file=project_root / MARKDOWN_FILES["progress"],
tasks_file=project_root / MARKDOWN_FILES["tasks"],
context_dir=project_root / ".erasmus" / "context",
protocols_dir=project_root / ".erasmus" / "protocols",
stored_protocols_dir=project_root / ".erasmus" / "protocols" / "stored",
registry_file=project_root / ".erasmus" / "protocols" / "registry.json",
markdown_files=MARKDOWN_FILES,
script_files=SCRIPT_FILES,
env_file=project_root / ".env",
_env_manager=env_manager,
)

return instance

def __init__(self, **data):
"""Initialize the SetupPaths instance."""
super().__init__(**data)
if not self._initialized:
self._initialized = True
# Ensure context directory exists
self.context_dir.mkdir(parents=True, exist_ok=True)

@property
def protocols_dir(self) -> Path:
"""Get the protocols directory path."""
return self.project_root / DIRECTORIES["protocols"]

@property
def stored_protocols_dir(self) -> Path:
"""Get the stored protocols directory path."""
return self.project_root / DIRECTORIES["stored_protocols"]

@property
def stored_context(self) -> Path:
"""Get the stored context directory path."""
return self.context_dir

@property
def active_context(self) -> Path:
"""Get the active context directory path."""
return self.context_dir

@property
def registry_file(self) -> Path:
"""Get the agent registry file path."""
return self.protocols_dir / "agent_registry.json"

@property
def markdown_files(self) -> Dict[str, Path]:
"""Get all markdown file paths."""
return {
"architecture": self.architecture_file,
"progress": self.progress_file,
"tasks": self.tasks_file,
}

@property
def script_files(self) -> Dict[str, Path]:
"""Get all script file paths."""
return {
"erasmus": self.project_root / SCRIPT_FILES["erasmus"],
}

def ensure_directories(self) -> None:
"""Ensure all required directories exist."""
directories = [
self.config_dir,
self.cache_dir,
self.logs_dir,
self.protocols_dir,
self.stored_protocols_dir,
self.stored_context,
]
for directory in directories:
directory.mkdir(parents=True, exist_ok=True)
logger.debug(f"Ensured directory exists: {directory}")

def model_dump(self) -> Dict[str, Any]:
"""Get a dictionary representation of all paths."""
return {
"project_root": self.project_root,
"rules_file": self.rules_file,
"global_rules_file": self.global_rules_file,
"config_dir": self.config_dir,
"cache_dir": self.cache_dir,
"logs_dir": self.logs_dir,
"protocols_dir": self.protocols_dir,
"stored_protocols_dir": self.stored_protocols_dir,
"registry_file": self.registry_file,
"markdown_files": self.markdown_files,
"stored_context": self.stored_context,
}

# Source: erasmus/utils/protocols.py

logger = get_logger(__name__)


class Protocol(BaseModel):
name: str
description: str
persona: str

PROTOCOL_PROJECT_OWNER = Protocol(
name="project_owner",
description="Project owner agent responsible for overseeing the core tracking documentation",
protocol="""

""")

PROTOCOL_DEVELOPER = Protocol(
name="developer",
description="Developer agent responsible for generating code",
protocol="""
""")

class ProtocolManager(BaseModel):
current_protocol: Protocol
protocols: dict[str, Protocol]

def __init__(self, protocols: dict[str, Protocol]):
self.protocols = protocols



# Source: erasmus/utils/__init__.py


# Source: erasmus/utils/env.py


# Source: erasmus/utils/context.py
"""Context management utilities for Erasmus."""





# Configure logging
logger = get_logger(__name__)

# Global variables
PWD = Path(__file__).parent
CLIENT = None
OPENAI_MODEL = None
PROJECT_MARKER = f"""
{"=" * 40}
## Current Project:
    {"=" * 40}
"""

SETUP_PATHS = SetupPaths.with_project_root(Path.cwd())
ENV_MANAGER = EnvironmentManager()


def extract_commit_message(message: str) -> str:
"""Extract the first line of a commit message and ensure it's properly formatted."""
# Get the first line
first_line = message.split("\n")[0].strip()

# Remove any quotes that might have been added
first_line = first_line.strip("\"'")

# Truncate if too long
if len(first_line) > 72:
first_line = first_line[:69] + "..."

return first_line


def determine_commit_type(diff_output: str) -> str:
"""Determine the commit type based on the diff content."""
diff_lower = diff_output.lower()

# Define patterns for different commit types
patterns = {
"test": ["test", "spec", "_test.py", "pytest"],
"fix": ["fix", "bug", "error", "issue", "crash", "problem"],
"docs": ["docs", "documentation", "readme", "comment"],
"style": ["style", "format", "lint", "pretty", "whitespace"],
"refactor": ["refactor", "restructure", "cleanup", "clean up", "simplify"],
"feat": ["feat", "feature", "add", "new", "implement"],
}

# Check each pattern
for commit_type, keywords in patterns.items():
if any(keyword in diff_lower for keyword in keywords):
return commit_type

# Default to chore
return "chore"


console = console.Console()

# Define ASCII character limit constant
ASCII_CHAR_LIMIT = 128


def scrub_non_ascii(text: str) -> str:
"""Remove non-standard ASCII characters from text.

Args:
text: The text to scrub

Returns:
str: The text with only standard ASCII characters
"""
return "".join(char for char in text if ord(char) < ASCII_CHAR_LIMIT)


class ProcessError(Exception):
"""Exception raised when a process fails."""


def read_file(path: Path) -> str:
"""Read a file and return its contents."""
try:
return path.read_text()
except Exception as e:
console.print(f"Error reading {path}: {e}", style="red")
return ""


def write_file(path: Path, content: str) -> bool:
"""Write content to a file"""
try:
path.parent.mkdir(parents=True, exist_ok=True)

path.write_text(content)
return True
except Exception as e:
console.print(f"Error writing to {path}: {e}", style="red")
return False


def get_rules_path() -> Path:
"""Get the appropriate rules file path based on IDE_ENV."""
return SETUP_PATHS.rules_file


def update_context(context: dict[str, Any]) -> dict[str, Any]:
"""Update the context with current file contents."""
# Add architecture if available
architecture_path = SETUP_PATHS.markdown_files["architecture"]
if architecture_path.exists():
context["architecture"] = read_file(architecture_path)

# Add progress if available
progress_path = SETUP_PATHS.markdown_files["progress"]
if progress_path.exists():
context["progress"] = read_file(progress_path)

# Add tasks if available
tasks_path = SETUP_PATHS.markdown_files["tasks"]
if tasks_path.exists():
context["tasks"] = read_file(tasks_path)

# Handle protocol context if a protocol is active
if "current_protocol" in context:
success = handle_protocol_context(SETUP_PATHS, context["current_protocol"])
if not success:
logger.warning(f"Failed to update protocol context for {context['current_protocol']}")

# Scrub non-ASCII characters only when writing to rules files
scrubbed_context = {
key: scrub_non_ascii(value) if isinstance(value, str) else value
for key, value in context.items()
}

# Get the correct rules file path based on IDE environment
rules_context_path = SETUP_PATHS.rules_file

# Write updated context, creating the file if it doesn't exist
write_file(rules_context_path, json.dumps(scrubbed_context, indent=2))
return context


def setup_project() -> None:
"""Set up a new project with necessary files."""

logger.info(f"Setup project detected IDE environment: '{ENV_MANAGER.ide_env}'")

setup_paths = SetupPaths.with_project_root(Path.cwd())

# Create required files
files = {
str(
setup_paths.markdown_files["architecture"]
): "# Project architecture\n\nDescribe your project architecture here.",
str(
setup_paths.markdown_files["progress"]
): "# Development progress\n\nTrack your development progress here.",
str(
setup_paths.markdown_files["tasks"]
): "# Project tasks\n\nList your project tasks here.",
".env.example": "IDE_ENV=\nOPENAI_API_KEY=\nOPENAI_BASE_URL=\nOPENAI_MODEL=",
".gitignore": ".env\n__pycache__/\n*.pyc\n.pytest_cache/\n",
}

# Add script files
for script_name, script_path in setup_paths.script_files.items():
files[str(script_path)] = f"""#!/usr/bin/env python3
# {script_name} script

def main():
# Add your {script_name} commands here
print(f"Running {script_name} script...")

if __name__ == "__main__":
main()
"""


@log_execution()
def update_specific_file(file_type: str, content: str | None = None) -> None:
"""Update a specific project file."""
with LogContext(logger, f"update_specific_file({file_type})"):
setup_paths = SetupPaths.with_project_root(Path.cwd())
file_map = setup_paths.markdown_files
# Get available file types from the MarkdownPaths object
available_types = list(file_map.keys())
logger.debug(f"Available file types: {available_types}")

if file_type not in available_types:
logger.error(f"Invalid file type: {file_type}")
console.print(f"Invalid file type: {file_type}", style="red")
return

# Access the path attribute directly based on the file_type
path = file_map[file_type]
logger.debug(f"Updating file: {path}")

# If no content provided, read from file
if content is None and path.exists():
try:
content = read_file(path)
logger.debug(f"Read existing content from {path}")
except Exception as e:
logger.error(f"Failed to read {path}: {e}", exc_info=True)
raise

if content is not None:
try:
if write_file(path, content):
logger.info(f"Successfully updated {path}")
if file_type != "context":
# Read current context
current_context = {}
rules_path = setup_paths.rules_file
if rules_path.exists():
try:
current_context = json.loads(read_file(rules_path))
logger.debug(
f"Read existing context: {list(current_context.keys())}"
)
except json.JSONDecodeError as e:
# If context file is invalid JSON, start fresh
logger.warning(f"Failed to parse context file, starting fresh: {e}")
current_context = {}

# Just update the content and save
current_context[file_type] = content
logger.info(f"üíæ Updating rules with changes from {file_type}")
write_file(rules_path, json.dumps(current_context, indent=2))
except Exception as e:
logger.error(f"Failed to update {path}: {e}", exc_info=True)
raise


@log_execution()
def cleanup_project() -> None:
"""Remove all generated files."""
with LogContext(logger, "cleanup_project"):
setup_paths = SetupPaths.with_project_root(Path.cwd())
rules_path = setup_paths.rules_file

files_to_remove = [
rules_path,
]
files_to_rm_string = [str(path) for path in files_to_remove]
files_to_rm_string.append(".env")
logger.debug(f"Files to clean up: {files_to_rm_string}")

# Then remove generated files
for filename in files_to_remove:
path = Path(filename)
if path.exists():
try:
path.unlink()
logger.info(f"Successfully removed {path}")
except Exception as e:
logger.error(f"Failed to remove {path}: {e}", exc_info=True)
console.print(f"Error removing {path}: {e}", style="red")
raise

# Remove cache directories
cache_patterns = [
"__pycache__",
".pytest_cache",
"*.pyc",
]
logger.debug(f"Cache patterns to clean: {cache_patterns}")

for pattern in cache_patterns:
for path in Path().rglob(pattern):
try:
if path.is_file():
path.unlink()
logger.debug(f"Removed cache file: {path}")
elif path.is_dir():
shutil.rmtree(path)
logger.debug(f"Removed cache directory: {path}")
except Exception as e:
logger.error(f"Failed to remove cache {path}: {e}", exc_info=True)
raise


@log_execution()
def check_creds() -> bool:
"""Check if OpenAI credentials are valid for API calls."""
with LogContext(logger, "check_creds"):
return ENV_MANAGER.get_openai_credentials()


@log_execution()
def make_atomic_commit() -> bool:
"""Makes an atomic commit with AI-generated commit message or falls back to diff-based message."""
with LogContext(logger, "make_atomic_commit"):
# Initialize GitManager with current directory
git_manager = GitManager(PWD)
logger.debug(f"Initialized GitManager with path: {PWD}")

# Stage all changes
if not git_manager.stage_all_changes():
logger.warning("No changes to commit or staging failed")
return False

try:
# Get the diff output
logger.debug("Getting staged diff output")
diff_output = subprocess.check_output(
["git", "diff", "--staged"],
cwd=PWD,
text=True,
universal_newlines=True,
encoding="utf-8",
errors="replace",  # Replace undecodable bytes
)

# Truncate diff if it's too long
max_diff_length = 4000
original_length = len(diff_output)
if original_length > max_diff_length:
diff_output = diff_output[:max_diff_length] + "... (diff truncated)"
logger.debug("Truncated diff from %s to %s chars", original_length, max_diff_length)

# Sanitize diff output
diff_output = "".join(char for char in diff_output if ord(char) < ASCII_CHAR_LIMIT)
logger.debug("Sanitized diff output, final length: %s", len(diff_output))

except Exception as e:
logger.exception("Failed to get diff output")
raise ProcessError("Failed to get diff output") from e

# Determine commit type programmatically
commit_type = determine_commit_type(diff_output)
logger.debug("Determined commit type: %s", commit_type)

# If we can use OpenAI, generate a message
if check_creds() and CLIENT is not None:
logger.debug("Using OpenAI to generate commit message")
prompt = f"""Generate a concise, descriptive commit message for the following git diff.
The commit type has been determined to be '{commit_type}'.

Diff:
    {diff_output}

    Guidelines:
        - Use the format: {commit_type}: description
- Keep message under 72 characters
- Be specific about the changes
- Prefer imperative mood"""

try:
logger.debug("Making API request with model: %s", OPENAI_MODEL)
response = CLIENT.chat.completions.create(
model=OPENAI_MODEL,
messages=[
{"role": "system", "content": "You are a git commit message generator."},
{"role": "user", "content": prompt},
],
max_tokens=100,
)

# Sanitize commit message
raw_message = response.choices[0].message.content
commit_message = "".join(
char for char in raw_message if ord(char) < ASCII_CHAR_LIMIT
)
logger.debug("Raw commit message: %s", raw_message)

# Ensure commit message starts with the determined type
if not commit_message.startswith(f"{commit_type}:"):
logger.debug("Adding commit type prefix")
commit_message = f"{commit_type}: {commit_message}"

commit_message = extract_commit_message(commit_message)
logger.info("Generated commit message via API: %s", commit_message)
except Exception as e:
logger.warning("Failed to generate commit message via API", exc_info=True)
commit_message = None
else:
logger.debug("Skipping OpenAI commit message generation")
commit_message = None

# If we don't have a commit message, generate one from the diff
if not commit_message:
logger.debug("Generating commit message from diff")
# Get the first line of the diff that shows a file change
diff_lines = diff_output.split("\n")
file_change = next((line for line in diff_lines if line.startswith("diff --git")), "")
if file_change:
# Extract the file name from the diff line
file_name = file_change.split(" b/")[-1]
logger.debug("Found changed file: %s", file_name)
else:
file_name = "project files"
logger.debug("No specific file changes found")

# Create a simple commit message based on the type and changed files
commit_message = f"{commit_type}: Update {file_name}"
logger.info("Generated fallback commit message: %s", commit_message)

# Validate commit message
is_valid, validation_message = git_manager.validate_commit_message(commit_message)
logger.debug("Commit message validation: %s", validation_message)

if not is_valid:
logger.warning("Generated commit message invalid: %s", validation_message)
commit_message = f"{commit_type}: Update project files ({time.strftime('%Y-%m-%d')})"
logger.info("Using default commit message: %s", commit_message)

# Commit changes
if git_manager.commit_changes(commit_message):
logger.info("Successfully committed changes: %s", commit_message)
return True
logger.error("Failed to commit changes")
return False


def sanitize_dirname(name: str) -> str:
"""Sanitize a string to be used as a directory name.

Args:
name: The string to sanitize

Returns:
str: A sanitized string suitable for use as a directory name
"""
# Remove any non-alphanumeric characters except hyphens and underscores
sanitized = "".join(c for c in name if c.isalnum() or c in "-_ ")
# Replace spaces with hyphens
sanitized = sanitized.replace(" ", "-")
# Remove any leading/trailing hyphens or underscores
sanitized = sanitized.strip("-_")
return sanitized




@snoop()
def store_context(setup_paths: SetupPaths) -> bool:
"""Store the current context.

Args:
setup_paths: The setup paths object containing file paths

Returns:
bool: True if context was stored successfully
"""
try:
# Read content from markdown files
architecture_content = read_file(setup_paths.architecture_file)
progress_content = read_file(setup_paths.progress_file)
tasks_content = read_file(setup_paths.tasks_file)

# Create context directory if it doesn't exist
context_dir = setup_paths.context_dir

architecture_title = architecture_content.split("\n")[0].strip("#").strip()
context_name = sanitize_dirname(architecture_title)

context_files = {
"architecture": architecture_content,
"progress": progress_content,
"tasks": tasks_content,
}

context_dir_path = context_dir / context_name
context_dir_path.mkdir(parents=True, exist_ok=True)

for filename, content in context_files.items():
target_path = context_dir_path / filename
write_file(target_path, content)

logger.info("Successfully stored context")
return True

except Exception as e:
logger.error(f"Failed to store context: {e}")
return False


def handle_agent_context(setup_paths: SetupPaths, context: str) -> bool:
"""Handles agent context"""
if not context.lower().strip("# ").startswith("agent"):
return context
current_architecture = setup_paths.markdown_files["architecture"].read_text()
if current_architecture.strip("# ").startswith("project"):
context += PROJECT_MARKER
context += current_architecture

return context


def restore_context(setup_paths: SetupPaths, context_path: Path | None = None) -> bool:
"""Restore the context from the context directory.

Args:
setup_paths: The setup paths object containing file paths
context_path: Optional path to the context directory to restore from.
If None, will use the default project_context directory.

Returns:
bool: True if context was restored successfully, False otherwise
"""
try:
# Get the context directory
if context_path is None:
context_dir = setup_paths.context_dir / "project_context"
else:
context_dir = Path(context_path)

# Define the context files with dot prefix in the context directory
context_architecture = context_dir / ".architecture.md"
context_progress = context_dir / ".progress.md"
context_tasks = context_dir / ".tasks.md"

# Read content from context files
architecture_content = (
context_architecture.read_text() if context_architecture.exists() else ""
)
progress_content = context_progress.read_text() if context_progress.exists() else ""
tasks_content = context_tasks.read_text() if context_tasks.exists() else ""

# Write content to root directory files
write_file(Path(".architecture.md"), architecture_content)
write_file(Path(".progress.md"), progress_content)
write_file(Path(".tasks.md"), tasks_content)

# Update the IDE rules file if it exists
if hasattr(setup_paths, "ide_rules_file"):
try:
# Read existing rules to preserve protocol information
existing_rules = {}
if setup_paths.ide_rules_file.exists():
try:
existing_rules = json.loads(setup_paths.ide_rules_file.read_text())
except json.JSONDecodeError:
logger.warning("Failed to parse existing rules file")

# Update content while preserving protocol information
rules_content = {
"architecture": architecture_content,
"progress": progress_content,
"tasks": tasks_content,
}

# Preserve protocol-related keys
protocol_keys = [
"current_protocol",
"protocol_role",
"protocol_triggers",
"protocol_produces",
"protocol_consumes",
"protocol_markdown",
"protocols",
]
for key in protocol_keys:
if key in existing_rules:
rules_content[key] = existing_rules[key]

write_file(setup_paths.ide_rules_file, json.dumps(rules_content, indent=2))
logger.debug(
"Updated IDE rules file with restored content while preserving protocol information"
)
except Exception as e:
logger.warning("Failed to update IDE rules file: %s", str(e))

logger.info("Context restored successfully from %s", context_dir)
return True
except Exception as e:
logger.error("Failed to restore context: %s", str(e), exc_info=True)
return False


def list_context_dirs(setup_paths: SetupPaths) -> list[str]:
"""List all context directories.

Args:
setup_paths: The setup paths object containing file paths

Returns:
list[str]: List of context directory paths as strings
"""
try:
context_dir = setup_paths.stored_context
if not context_dir.exists():
logger.debug("Context directory does not exist: %s", context_dir)
return []

# Get all directories that contain the required context files
valid_dirs = []
for d in context_dir.iterdir():
# Check for any directory that contains the required files
if d.is_dir():
# Check for the required files in the directory
files_to_check = [".architecture.md", ".progress.md", ".tasks.md"]
if all((d / f).exists() for f in files_to_check):
valid_dirs.append(str(d))

if not valid_dirs:
logger.debug("No valid context directories found in %s", context_dir)
return valid_dirs

logger.debug("Found %d valid context directories", len(valid_dirs))
return valid_dirs
except Exception:
logger.exception("Error listing context directories")
return []


def print_context_dirs(setup_paths: SetupPaths) -> None:
"""Print all context directories.

Args:
setup_paths: The setup paths object containing file paths
"""
context_dirs = list_context_dirs(setup_paths)
if not context_dirs:
console.print("No context directories found")
return

console.print("Available context directories:")
for i, context_dir in enumerate(context_dirs, 1):
# Try to extract a more readable name from the path
dir_name = Path(context_dir).name
# Remove the "Project-" prefix for display
display_name = dir_name.replace("Project-", "")
# Get the first line of .architecture.md as title if possible
architecture_file = Path(context_dir) / ".architecture.md"
if architecture_file.exists():
try:
with architecture_file.open() as f:
first_line = f.readline().strip("#").strip()
if first_line:
display_name = f"{display_name} - {first_line}"
except Exception:
pass  # Just use the directory name if we can't read the file

console.print(f"{i}. {display_name}")


def select_context_dir(setup_paths: SetupPaths) -> Path | None:
"""Select a context directory.

Args:
setup_paths: The setup paths object containing file paths

Returns:
Path | None: The selected context directory path, or None if no selection was made
"""
context_dirs = list_context_dirs(setup_paths)
if not context_dirs:
console.print("No context directories found")
return None

print_context_dirs(setup_paths)

while True:
try:
prompt = "Enter the number of the context directory to restore (or 'q' to quit): "
selection_input = console.input(prompt)

if selection_input.lower() in ("q", "quit", "exit"):
return None

try:
selection = int(selection_input)
if 1 <= selection <= len(context_dirs):
return Path(context_dirs[selection - 1])

msg = f"Invalid selection. Enter a number between 1 and {len(context_dirs)}."
console.print(msg)
except ValueError:
console.print("Invalid input, please enter a number or 'q' to quit.")
except KeyboardInterrupt:
console.print("\nSelection cancelled")
return None


def handle_protocol_context(setup_paths: SetupPaths, protocol_name: str) -> bool:
"""Handle protocol context updates automatically.

This function acts as an event handler for protocol state changes,
automatically updating the context when protocols change state.

Args:
setup_paths: The setup paths object containing file paths
protocol_name: Name of the protocol being activated

Returns:
bool: True if context was updated successfully
"""
try:
# Get the rules file path
rules_path = setup_paths.rules_file
if not rules_path.exists():
logger.warning("Rules file does not exist, creating new context")
rules_context = {}
else:
with open(rules_path) as f:
rules_context = json.loads(f.read())

# Update protocol state in context
rules_context["current_protocol"] = protocol_name

# Let the ProtocolManager handle the protocol state
async def update_protocol_state():
manager = await ProtocolManager.create()
protocol = manager.get_protocol(protocol_name)

if protocol:
# Update protocol metadata
rules_context.update(
{
"protocol_role": protocol.role,
"protocol_triggers": protocol.triggers,
"protocol_produces": protocol.produces,
"protocol_consumes": protocol.consumes,
}
)

# Load protocol markdown
protocol_md_path = setup_paths.protocols_dir / "stored" / protocol.file_path.name
if protocol_md_path.exists():
rules_context["protocol_markdown"] = read_file(protocol_md_path)

# Update available protocols
rules_context["protocols"] = manager.list_protocols()

return True
return False

success = asyncio.run(update_protocol_state())
if not success:
logger.error(f"Failed to update protocol state for {protocol_name}")
return False

# Write updated context
write_file(rules_path, json.dumps(rules_context, indent=2))
logger.info(f"Successfully updated context for protocol: {protocol_name}")
return True

except Exception as e:
logger.error(f"Error handling protocol context: {e}")
return False


class ContextManager:
"""Manages the global context for the application."""

def __init__(self):
"""Initialize the context manager."""
self.context = Context()
self.protocol_manager = None
self._setup_protocol_handlers()

def _setup_protocol_handlers(self) -> None:
"""Set up handlers for protocol events."""
if self.protocol_manager:
self.protocol_manager.register_event_handler(
"protocol_activated", self._handle_protocol_activated
)
self.protocol_manager.register_event_handler(
"protocol_completed", self._handle_protocol_completed
)
self.protocol_manager.register_event_handler(
"transition_triggered", self._handle_transition
)
self.protocol_manager.register_event_handler("artifact_produced", self._handle_artifact)

async def initialize(self) -> None:
"""Initialize the context manager."""
self.protocol_manager = await ProtocolManager.create()
self._setup_protocol_handlers()

def _handle_protocol_activated(self, protocol: Protocol) -> None:
"""Handle protocol activation event."""
self.context.active_protocol = protocol.name
self.context.protocol_state = protocol.initial_state
logger.info(f"Activated protocol: {protocol.name}")

def _handle_protocol_completed(self, protocol: Protocol, artifacts: Dict[str, Any]) -> None:
"""Handle protocol completion event."""
self.context.protocol_artifacts.update(artifacts)
logger.info(f"Completed protocol: {protocol.name}")

def _handle_transition(
self, from_protocol: Protocol, to_protocol: Protocol, trigger: str, artifact: str
) -> None:
"""Handle protocol transition event."""
logger.info(f"Transitioning from {from_protocol.name} to {to_protocol.name}")
logger.info(f"Trigger: {trigger}, Artifact: {artifact}")

# Update context for new protocol
self.context.active_protocol = to_protocol.name
self.context.protocol_state = to_protocol.initial_state

def _handle_artifact(self, protocol: Protocol, artifact_name: str, artifact_value: Any) -> None:
"""Handle artifact production event."""
self.context.protocol_artifacts[artifact_name] = artifact_value
logger.info(f"Produced artifact {artifact_name} in protocol {protocol.name}")

async def update_context(self, updates: Dict[str, Any]) -> None:
"""Update the context with new values.

Args:
updates: Dictionary of context updates
"""
# Update context values
for key, value in updates.items():
if hasattr(self.context, key):
setattr(self.context, key, value)
else:
logger.warning(f"Unknown context key: {key}")

# If we have an active protocol, check for completion
if self.context.active_protocol and self.protocol_manager:
protocol = self.protocol_manager.get_protocol(self.context.active_protocol)
if protocol and protocol.is_complete(self.context.protocol_state):
await self.protocol_manager.complete_protocol(
protocol.name, self.context.protocol_artifacts
)

def get_context(self) -> Context:
"""Get the current context."""
return self.context

async def select_protocol(self, protocol_name: str) -> bool:
"""Select and activate a protocol.

Args:
protocol_name: Name of the protocol to activate

Returns:
bool: True if protocol was activated successfully
"""
if not self.protocol_manager:
logger.error("Protocol manager not initialized")
return False

return await self.protocol_manager.activate_protocol(protocol_name)

def read_context(self) -> dict[str, Any]:
"""Read and parse the context file.

Returns:
Dict containing context configuration
"""
try:
if not self.context_file.exists():
return {
"project_root": str(self.workspace_root),
"active_rules": [],
"global_rules": [],
"file_patterns": ["*.py", "*.md"],
"excluded_paths": ["venv/", "__pycache__/"],
}

content = self.context_file.read_text()
context = json.loads(content)
return context

except json.JSONDecodeError:
# If JSON is invalid, return default context
return {
"project_root": str(self.workspace_root),
"active_rules": [],
"global_rules": [],
"file_patterns": ["*.py", "*.md"],
"excluded_paths": ["venv/", "__pycache__/"],
}
except Exception as e:
logger.error(f"Failed to read context file: {e}")
return {
"project_root": str(self.workspace_root),
"active_rules": [],
"global_rules": [],
"file_patterns": ["*.py", "*.md"],
"excluded_paths": ["venv/", "__pycache__/"],
}

def update_context(self, new_context: dict[str, Any], partial: bool = False) -> None:
"""Update the context file.

Args:
new_context: New context configuration
partial: If True, only update specified fields
"""
try:
if partial:
current_context = self.read_context()
current_context.update(new_context)
new_context = current_context

# Write the updated context
self.context_file.write_text(json.dumps(new_context, indent=2))
except Exception as e:
logger.error(f"Failed to update context: {e}")

# Source: erasmus/utils/file_ops.py
"""File operation utilities."""

logger = logging.getLogger(__name__)

def safe_write_file(file_path: Path, content: str) -> None:
"""
Safely write content to a file using a temporary file to ensure atomic writes.

Args:
file_path: Path to the target file
content: Content to write to the file
"""
# Create parent directory if it doesn't exist
file_path.parent.mkdir(parents=True, exist_ok=True)

# Create a temporary file in the same directory
temp_fd, temp_path = tempfile.mkstemp(dir=str(file_path.parent))
try:
with os.fdopen(temp_fd, 'w') as f:
f.write(content)

# On Windows, we need to remove the target file first
if os.name == 'nt' and file_path.exists():
file_path.unlink()

# Rename temporary file to target file (atomic on Unix)
Path(temp_path).replace(file_path)
except Exception:
# Clean up temp file if something goes wrong
with contextlib.suppress(OSError):
Path(temp_path).unlink()
raise

def safe_read_file(file_path: Path) -> str:
"""
Safely read content from a file.

Args:
file_path: Path to the file to read

Returns:
The content of the file as a string

Raises:
FileNotFoundError: If the file doesn't exist
"""
try:
with file_path.open() as f:
return f.read()
except Exception:
logger.exception("Error reading file %s", file_path)
raise

# Source: erasmus/utils/protocols/context.py
"""Context class for protocol state management."""



class Context:
"""Context class for managing protocol state."""

def __init__(self):
self.active_protocol: Optional[str] = None
self.protocol_state: Dict[str, Any] = {}
self.protocol_artifacts: Dict[str, Any] = {}

# Source: erasmus/utils/protocols/base.py


logger = get_logger(__name__)


class ProtocolArtifact(BaseModel):
"""Represents an artifact produced by a protocol."""

model_config = ConfigDict(arbitrary_types_allowed=True)

name: str
content: Any
path: Optional[str] = None


class ProtocolTransition(BaseModel):
"""Represents a transition between protocols."""

model_config = ConfigDict(arbitrary_types_allowed=True)

from_agent: str
to_agent: str
trigger: str
artifact: str
condition: Callable[[Dict[str, Any]], bool] = Field(default=lambda _: True)


class Protocol(BaseModel):
"""Base class for all protocols."""

model_config = ConfigDict(arbitrary_types_allowed=True)

name: str
role: str
triggers: List[str] = Field(default_factory=list)
produces: List[str] = Field(default_factory=list)
consumes: List[str] = Field(default_factory=list)
markdown: str = ""
artifacts: List[ProtocolArtifact] = Field(default_factory=list)
transitions: List[ProtocolTransition] = Field(default_factory=list)
prompt: Optional[Callable[[Dict[str, Any]], Any]] = None
file_path: Optional[Path] = None

async def execute(self, context: Dict[str, Any]) -> List[ProtocolArtifact]:
"""Execute the protocol with the given context."""
if not self.prompt:
raise ValueError(f"No prompt function registered for protocol: {self.name}")

# Ensure context is a dictionary
if not isinstance(context, dict):
context = {"context": context}

# Add protocol-specific context
context["protocol_name"] = self.name
context["protocol_role"] = self.role

# Execute the prompt function
prompt_result = await self.prompt(context)

# Process artifacts
artifacts = []

# If prompt_result is a string, treat it as markdown content
if isinstance(prompt_result, str):
# Create a markdown artifact
artifacts.append(
ProtocolArtifact(
name=f"{self.name.lower().replace(' ', '_')}.md",
content=prompt_result,
path=f"erasmus/utils/protocols/stored/{self.name.lower().replace(' ', '_')}.md",
)
)
# If prompt_result is a dictionary, create artifacts for each key
elif isinstance(prompt_result, dict):
for key, value in prompt_result.items():
artifacts.append(
ProtocolArtifact(
name=key, content=value, path=f"erasmus/utils/protocols/stored/{key}"
)
)
# If prompt_result is a list, create artifacts for each item
elif isinstance(prompt_result, list):
for i, item in enumerate(prompt_result):
artifacts.append(
ProtocolArtifact(
name=f"{self.name.lower().replace(' ', '_')}_{i}",
content=item,
path=f"erasmus/utils/protocols/stored/{self.name.lower().replace(' ', '_')}_{i}",
)
)
# If prompt_result is None or something else, create a default artifact
else:
artifacts.append(
ProtocolArtifact(
name=f"{self.name.lower().replace(' ', '_')}",
content=str(prompt_result),
path=f"erasmus/utils/protocols/stored/{self.name.lower().replace(' ', '_')}",
)
)

# Add any predefined artifacts from the protocol
for artifact in self.artifacts:
if artifact.name in context:
artifacts.append(
ProtocolArtifact(
name=artifact.name, content=context[artifact.name], path=artifact.path
)
)

return artifacts

def get_transitions(self, context: Dict[str, Any]) -> List[ProtocolTransition]:
"""Get available transitions based on context."""
# Ensure context is a dictionary
if not isinstance(context, dict):
context = {"context": context}

# Add protocol-specific context
context["protocol_name"] = self.name
context["protocol_role"] = self.role

return [t for t in self.transitions if t.condition(context)]

# Source: erasmus/utils/protocols/manager.py
"""Protocol manager for loading and executing protocols."""





logger = get_logger(__name__)


class ProtocolRegistry(BaseModel):
"""Registry containing all protocols and their transitions."""

model_config = ConfigDict(arbitrary_types_allowed=True)

agents: List[Protocol] = Field(default_factory=list)
workflow_transitions: List[ProtocolTransition] = Field(default_factory=list)


class ProtocolManager(BaseModel):
"""Manages protocol files and their registration."""

path_manager: PathManager = Field(default_factory=PathManager)
registry: Dict[str, Any] = Field(default_factory=dict)
event_handlers: Dict[str, List[Callable]] = Field(default_factory=dict)
prompt_functions: Dict[str, Callable] = Field(default_factory=dict)
transitions: List[ProtocolTransition] = Field(default_factory=list)
model_config = ConfigDict(arbitrary_types_allowed=True)

def __init__(self, **data):
"""Initialize the ProtocolManager."""
if "path_manager" not in data:
data["path_manager"] = PathManager()
super().__init__(**data)

@classmethod
async def create(cls) -> "ProtocolManager":
"""Create a new ProtocolManager instance."""
instance = cls()
try:
instance.path_manager.ensure_directories()
await instance.load_registry()
await instance.register_default_prompts()
except Exception as e:
logger.error(f"Error creating ProtocolManager: {e}")
# Optionally, you could re-raise the exception or handle it differently
raise
return instance

async def load_registry(self) -> Dict[str, Any]:
"""Load the protocol registry from disk."""
registry_file = self.path_manager.registry_file
if not registry_file.exists():
logger.info(f"Registry file not found at {registry_file}, creating new registry")
self.registry = {}
return self.registry
try:
with open(registry_file, "r") as f:
self.registry = json.load(f)
return self.registry
except Exception as e:
logger.error(f"Failed to load registry from {registry_file}: {e}")
self.registry = {}
return self.registry

async def register_default_prompts(self) -> None:
"""Register default prompt functions for all protocols."""
protocols_dir = self.path_manager.protocols_dir / "stored"
registry_file = self.path_manager.registry_file

# Ensure the protocols directory exists
if not protocols_dir.exists():
logger.warning(f"Protocols directory not found: {protocols_dir}")
return

# Find all markdown files in the stored protocols directory
protocol_files = list(protocols_dir.glob("*.md"))

if not protocol_files:
logger.info("No protocol files found in stored protocols directory")
return

for protocol_file in protocol_files:
protocol_name = protocol_file.stem

try:
# Read the protocol content from the file
with open(protocol_file, "r") as f:
protocol_content = f.read().strip()

# If protocol not in registry, add it
if protocol_name not in self.registry:
self.registry[protocol_name] = {
"name": protocol_name,
"description": f"Default protocol for {protocol_name}",
"file_path": str(protocol_file),
"protocol_content": protocol_content,
}
logger.info(f"Added protocol {protocol_name} to registry")

except IOError as e:
logger.error(f"Error reading protocol file {protocol_file}: {e}")
except Exception as e:
logger.error(f"Unexpected error processing protocol {protocol_name}: {e}")

# Optionally save the updated registry
try:
# Ensure the directory exists
registry_file.parent.mkdir(parents=True, exist_ok=True)

# Write the serializable registry to the file
with open(registry_file, "w") as f:
serializable_registry = {}
for name, protocol in self.registry.items():
# Ensure protocol is a dictionary or has model_dump method
if hasattr(protocol, "model_dump"):
protocol_dict = protocol.model_dump()
elif not isinstance(protocol, dict):
protocol_dict = {"name": name, "description": "", "file_path": ""}
else:
protocol_dict = protocol

# Convert PosixPath to string
serializable_registry[name] = {
"name": protocol_dict.get("name", name),
"description": protocol_dict.get("description", ""),
"file_path": str(protocol_dict.get("file_path", "")),
"protocol_content": protocol_dict.get("protocol_content", ""),
}
json.dump(serializable_registry, f, indent=2)

logger.info(f"Registry saved to {registry_file}")
except Exception as e:
logger.error(f"Failed to save updated registry: {e}")

async def load_registry(self) -> None:
"""Load the protocol registry from the registry file."""
registry_path = self.path_manager.registry_file
if not registry_path.exists():
logger.info(f"Creating new registry file at: {registry_path}")
registry_data = {"agents": [], "workflow_transitions": []}
with open(registry_path, "w") as f:
json.dump(registry_data, f, indent=2)
return

try:
with open(registry_path, "r") as f:
registry_data = json.load(f)

# Ensure stored protocols directory exists
stored_protocols_dir = self.path_manager.stored_protocols_dir
stored_protocols_dir.mkdir(parents=True, exist_ok=True)

for agent in registry_data.get("agents", []):
# Sanitize protocol name for file path
safe_name = agent["name"].replace("/", "_").replace("\\", "_").strip()

# Check for existing protocol file
protocol_file = None

# First check in the stored protocols directory
stored_file = stored_protocols_dir / f"{safe_name}.md"
if stored_file.exists():
protocol_file = stored_file
logger.info(f"Found existing protocol file: {protocol_file}")

# If not found in stored directory, check in the default protocols directory
if not protocol_file:
default_file = self.path_manager.protocols_dir / f"{safe_name}.md"
if default_file.exists():
protocol_file = default_file
logger.info(f"Found protocol file in default directory: {protocol_file}")

# If still not found, create a new file in the stored directory
if not protocol_file:
protocol_file = stored_file
# Create a basic protocol template
protocol_content = f"""# {agent["name"]}

## Role
{agent["role"]}

## Triggers
{", ".join(agent.get("triggers", []))}

## Produces
{", ".join(agent.get("produces", []))}

## Consumes
{", ".join(agent.get("consumes", []))}

## Description
This is a protocol for the {agent["name"]} role.
"""
protocol_file.write_text(protocol_content)
logger.info(f"Created new protocol file: {protocol_file}")

protocol = Protocol(
name=agent["name"],  # Keep original name for display
role=agent["role"],
triggers=agent.get("triggers", []),
produces=agent.get("produces", []),
consumes=agent.get("consumes", []),
file_path=protocol_file,
)
self.registry[protocol.name] = protocol
logger.info(f"Loaded protocol: {protocol.name}")

# Load workflow transitions
self.transitions = [
ProtocolTransition(**transition)
for transition in registry_data.get("workflow_transitions", [])
]
logger.info(f"Loaded {len(self.transitions)} workflow transitions")

except Exception as e:
logger.error(f"Error loading registry: {e}")

def register_prompt_function(self, protocol_name: str, prompt_function: Callable) -> None:
"""Register a prompt function for a protocol."""
if protocol_name not in self.registry:
logger.warning(f"Cannot register prompt function for unknown protocol: {protocol_name}")
return

self.prompt_functions[protocol_name] = prompt_function
logger.info(f"Registered prompt function for protocol: {protocol_name}")

def save_registry(self) -> None:
"""Save the protocol registry to a JSON file."""
try:
# Create a serializable version of the registry
serializable_registry = {}
for name, protocol in self.registry.items():
# Convert Protocol objects to dictionaries
if hasattr(protocol, "model_dump"):
serializable_registry[name] = protocol.model_dump()
else:
serializable_registry[name] = {
"name": protocol.get("name", name),
"description": protocol.get("description", ""),
"file_path": protocol.get("file_path", ""),
}

# Ensure the directory exists
registry_file = self.path_manager.agent_registry_file
registry_file.parent.mkdir(parents=True, exist_ok=True)

# Write the serializable registry to the file
with open(registry_file, "w") as f:
json.dump(serializable_registry, f, indent=2)

logger.info(f"Registry saved to {registry_file}")
except Exception as e:
logger.error(f"Failed to save registry to {registry_file}: {e}")

def register_protocol(self, name: str, description: str, file_path: Path) -> None:
"""Register a new protocol in the registry."""
if name in self.registry:
logger.warning(f"Protocol {name} already exists in registry")
return

self.registry[name] = {
"description": description,
"file_path": str(file_path),
"created_at": datetime.now().isoformat(),
}
self.save_registry()
logger.info(f"Registered protocol {name}")

def unregister_protocol(self, name: str) -> None:
"""Remove a protocol from the registry."""
if name not in self.registry:
logger.warning(f"Protocol {name} not found in registry")
return

del self.registry[name]
self.save_registry()
logger.info(f"Unregistered protocol {name}")

def get_protocol(self, name: str) -> Optional[Dict[str, Any]]:
"""Get a protocol from the registry by name."""
return self.registry.get(name)

def list_protocols(self) -> List[Dict[str, Any]]:
"""List all registered protocols."""
return list(self.registry.values())

def get_protocol_file(self, name: str) -> Optional[Path]:
"""Get the file path for a protocol by name."""
protocol = self.get_protocol(name)
if not protocol:
return None
return Path(protocol.get("file_path", ""))

def get_protocol_json(self, name: str) -> Optional[Path]:
"""Get the JSON file path for a protocol by name."""
protocol = self.get_protocol(name)
if not protocol:
return None
file_path = Path(protocol.get("file_path", ""))
return file_path.with_suffix(".json")

def register_event_handler(self, event_type: str, handler: Callable) -> None:
"""Register an event handler for a specific event type."""
if event_type not in self.event_handlers:
self.event_handlers[event_type] = []
self.event_handlers[event_type].append(handler)

def _emit_event(self, event_type: str, data: Any) -> None:
"""Emit an event to all registered handlers."""
if event_type in self.event_handlers:
for handler in self.event_handlers[event_type]:
handler(data)

async def execute_protocol(self, name: str, context: dict) -> List[ProtocolTransition]:
"""Execute a protocol and return possible next transitions."""
protocol = self.get_protocol(name)
if not protocol:
raise ValueError(f"Protocol not found: {name}")

# Ensure context is a dictionary
if not isinstance(context, dict):
context = {"context": context}

# Add protocol-specific context
context["protocol_name"] = protocol["name"]
context["protocol_role"] = protocol["role"]

# Execute the protocol
artifacts = await protocol.execute(context)

# Find possible next transitions based on produced artifacts
possible_transitions = []
for artifact in artifacts:
transitions = [
t
for t in self.transitions
if t.from_agent == protocol["name"] and t.artifact == artifact.name
]
possible_transitions.extend(transitions)

return possible_transitions

def get_protocol(self, protocol_name: str) -> Optional[Dict[str, Any]]:
"""Get a protocol by name."""
return self.registry.get(protocol_name)

def list_protocols(self) -> List[Dict[str, Any]]:
"""List all available protocols."""
return list(self.registry.values())

def get_transitions_from(self, protocol_name: str) -> List[ProtocolTransition]:
"""Get all transitions originating from a protocol."""
return [t for t in self.transitions if t.from_agent == protocol_name]

def get_transitions_to(self, protocol_name: str) -> List[ProtocolTransition]:
"""Get all transitions targeting a protocol."""
return [t for t in self.transitions if t.to_agent == protocol_name]

async def activate_protocol(self, protocol_name: str) -> bool:
"""Activate a protocol by name."""
try:
# Check if the protocol exists in the registry
if protocol_name not in self.registry:
logger.warning(f"Protocol {protocol_name} not found in registry")
return False

# Perform any necessary activation steps
# This could involve setting a default agent, initializing resources, etc.
logger.info(f"Activating protocol: {protocol_name}")

return True
except Exception as e:
logger.error(f"Error activating protocol {protocol_name}: {e}")
return False

async def complete_protocol(self, protocol_name: str, result: Dict[str, Any]) -> bool:
"""Complete a protocol."""
protocol = self.get_protocol(protocol_name)
if not protocol:
logger.error(f"Cannot complete unknown protocol: {protocol_name}")
return False

await self._emit_event("protocol_completed", {"protocol": protocol, "result": result})
return True

def get_transitions(self, protocol_name: str) -> List[ProtocolTransition]:
"""Get all transitions for a protocol."""
return [t for t in self.transitions if t.from_agent == protocol_name]

def get_protocol_transitions(
self, protocol_name: str, context: Dict[str, Any]
) -> List[ProtocolTransition]:
"""Get available transitions for a protocol based on context."""
if protocol_name not in self.registry:
raise ValueError(f"Unknown protocol: {protocol_name}")

# Ensure context is a dictionary
if not isinstance(context, dict):
context = {"context": context}

# Add protocol-specific context
context["protocol_name"] = protocol_name
context["protocol_role"] = self.registry[protocol_name]["role"]

return [
t for t in self.transitions if t.from_agent == protocol_name and t.condition(context)
]

# Source: erasmus/utils/protocols/server.py


logger = logging.getLogger(__name__)


class ProtocolExecutionRequest(BaseModel):
"""Request model for protocol execution."""

protocol_name: str
context: dict


class ProtocolExecutionResponse(BaseModel):
"""Response model for protocol execution."""

artifacts: List[ProtocolArtifact]
next_transitions: List[ProtocolTransition]


class ProtocolServer:
"""Server for managing protocol execution and transitions."""

def __init__(self, setup_paths: Optional[SetupPaths] = None):
"""Initialize the protocol server."""
self.setup_paths = setup_paths or SetupPaths.with_project_root(Path.cwd())
self.protocol_manager = None
self.logger = logging.getLogger(__name__)

async def initialize(self) -> None:
"""Initialize the protocol server."""
self.protocol_manager = await ProtocolManager.create()
self._register_default_handlers()

def _register_default_handlers(self) -> None:
"""Register default event handlers."""
self.protocol_manager.register_event_handler(
"protocol_activated", self._handle_protocol_activated
)
self.protocol_manager.register_event_handler(
"protocol_completed", self._handle_protocol_completed
)
self.protocol_manager.register_event_handler(
"transition_triggered", self._handle_transition
)
self.protocol_manager.register_event_handler("artifact_produced", self._handle_artifact)

def _handle_protocol_activated(self, data: Dict[str, Any]) -> None:
"""Handle protocol activation event."""
protocol_name = data.get("protocol_name")
self.logger.info(f"Protocol activated: {protocol_name}")

def _handle_protocol_completed(self, data: Dict[str, Any]) -> None:
"""Handle protocol completion event."""
protocol_name = data.get("protocol_name")
self.logger.info(f"Protocol completed: {protocol_name}")

def _handle_transition(self, data: Dict[str, Any]) -> None:
"""Handle transition event."""
from_protocol = data.get("from_protocol")
to_protocol = data.get("to_protocol")
self.logger.info(f"Transition: {from_protocol} -> {to_protocol}")

def _handle_artifact(self, data: Dict[str, Any]) -> None:
"""Handle artifact production event."""
protocol_name = data.get("protocol_name")
artifact_type = data.get("artifact_type")
self.logger.info(f"Artifact produced: {artifact_type} from {protocol_name}")

async def execute_protocol(
self, protocol_name: str, context: Dict[str, Any]
) -> List[ProtocolArtifact]:
"""Execute a protocol with the given context."""
if not self.protocol_manager:
await self.initialize()

protocol = self.protocol_manager.get_protocol(protocol_name)
if not protocol:
raise ValueError(f"Protocol not found: {protocol_name}")

await self.protocol_manager.activate_protocol(protocol_name)
artifacts = await self.protocol_manager.execute_protocol(protocol_name, context)
await self.protocol_manager.complete_protocol(protocol_name, {"artifacts": artifacts})
return artifacts

def get_protocol(self, protocol_name: str) -> Optional[Protocol]:
"""Get a protocol by name."""
return self.protocol_manager.get_protocol(protocol_name) if self.protocol_manager else None

def list_protocols(self) -> List[Protocol]:
"""List all available protocols."""
return self.protocol_manager.list_protocols() if self.protocol_manager else []

def get_protocol_transitions(
self, protocol_name: str, context: Dict[str, Any]
) -> List[ProtocolTransition]:
"""Get available transitions for a protocol."""
return (
self.protocol_manager.get_protocol_transitions(protocol_name, context)
if self.protocol_manager
else []
)

# Source: erasmus/utils/protocols/integration.py


logger = get_logger(__name__)


class ProtocolIntegration:
"""Integration layer for the protocol system."""

def __init__(self, setup_paths: SetupPaths = None):
"""Initialize the protocol integration.

Args:
setup_paths: Optional SetupPaths instance. If not provided, will create one with current directory.
"""
self.protocol_server = None
self.setup_paths = setup_paths or SetupPaths.with_project_root(Path.cwd())
self.registry_path = self.setup_paths.protocols_dir / "agent_registry.json"
self.current_protocol = None
self.context = Context()
self._register_default_handlers()

def _register_default_handlers(self):
"""Register default event handlers."""
self._handlers = {
"protocol_activated": self._handle_protocol_activated,
"protocol_completed": self._handle_protocol_completed,
"transition": self._handle_transition,
"artifact": self._handle_artifact,
}

def _handle_protocol_activated(self, data: Dict[str, Any]):
"""Handle protocol activation event."""
logger.info(f"Protocol activated: {data['protocol']}")
self.current_protocol = data["protocol"]
self.context.active_protocol = data["protocol"]

def _handle_protocol_completed(self, data: Dict[str, Any]):
"""Handle protocol completion event."""
logger.info(f"Protocol completed: {data['protocol']}")
self.current_protocol = None
self.context.active_protocol = None

def _handle_transition(self, data: Dict[str, Any]):
"""Handle transition event."""
logger.info(f"Transition: {data['from_protocol']} -> {data['to_protocol']}")
self.current_protocol = data["to_protocol"]
self.context.active_protocol = data["to_protocol"]

def _handle_artifact(self, data: Dict[str, Any]):
"""Handle artifact event."""
logger.info(f"Artifact produced: {data['artifact'].name}")
self.context.protocol_artifacts[data["artifact"].name] = data["artifact"].content

async def initialize(self) -> None:
"""Initialize the protocol system."""
if not self.registry_path.exists():
logger.error(f"Registry file not found: {self.registry_path}")
return

self.protocol_server = ProtocolServer(self.registry_path)
logger.info("Protocol system initialized")

def register_protocol_prompts(self) -> None:
"""Register prompt functions for protocols."""
if not self.protocol_server:
logger.error("Protocol server not initialized")
return

# Register default prompt functions
self.protocol_server.protocol_manager.register_default_prompts()
logger.info("Protocol prompts registered")

async def execute_protocol(
self, protocol_name: str, context: Dict[str, Any]
) -> List[ProtocolArtifact]:
"""Execute a protocol with the given context."""
if not self.protocol_server:
logger.error("Protocol server not initialized")
return []

try:
# Manually trigger protocol activation
self._handle_protocol_activated({"protocol": protocol_name})

artifacts = await self.protocol_server.execute_protocol(protocol_name, context)

# Write file artifacts to disk
for artifact in artifacts:
if artifact.type == "file":
artifact_path = self.setup_paths.protocols_dir / "stored" / artifact.name
artifact_path.write_text(artifact.content)
logger.info(f"Wrote artifact to {artifact_path}")

# Manually trigger protocol completion
self._handle_protocol_completed({"protocol": protocol_name})

return artifacts
except Exception as e:
logger.error(f"Error executing protocol {protocol_name}: {e}")
return []

def get_protocol(self, protocol_name: str) -> Optional[Protocol]:
"""Get a protocol by name."""
if not self.protocol_server:
logger.error("Protocol server not initialized")
return None

return self.protocol_server.get_protocol(protocol_name)

def list_protocols(self) -> List[str]:
"""List all available protocols."""
if not self.protocol_server:
logger.error("Protocol server not initialized")
return []

return self.protocol_server.list_protocols()

def get_protocol_transitions(
self, protocol_name: str, context: Dict[str, Any]
) -> List[ProtocolTransition]:
"""Get available transitions for a protocol based on context."""
if not self.protocol_server:
logger.error("Protocol server not initialized")
return []

return self.protocol_server.get_protocol_transitions(protocol_name, context)

async def transition_to_protocol(
self, from_protocol: str, to_protocol: str, context: Dict[str, Any]
) -> bool:
"""Manually transition from one protocol to another."""
if not self.protocol_server:
logger.error("Protocol server not initialized")
return False

try:
# Verify the transition is valid
transitions = self.get_protocol_transitions(from_protocol, context)
valid_transition = any(t.to_protocol == to_protocol for t in transitions)

if not valid_transition:
logger.error(f"Invalid transition from {from_protocol} to {to_protocol}")
return False

# Manually trigger the transition
self._handle_transition({"from_protocol": from_protocol, "to_protocol": to_protocol})

return True
except Exception as e:
logger.error(f"Error transitioning from {from_protocol} to {to_protocol}: {e}")
return False

async def run_workflow(
self, start_protocol: str, context: Dict[str, Any]
) -> List[ProtocolArtifact]:
"""Run a workflow starting from a specific protocol."""
if not self.protocol_server:
logger.error("Protocol server not initialized")
return []

try:
artifacts = []
current_protocol = start_protocol

while current_protocol:
# Execute current protocol
protocol_artifacts = await self.execute_protocol(current_protocol, context)
artifacts.extend(protocol_artifacts)

# Get next transitions
transitions = self.get_protocol_transitions(current_protocol, context)
if not transitions:
break

# Select first transition and manually transition
next_protocol = transitions[0].to_protocol
success = await self.transition_to_protocol(
current_protocol, next_protocol, context
)
if not success:
break

current_protocol = next_protocol

return artifacts
except Exception as e:
logger.error(f"Error running workflow from {start_protocol}: {e}")
return []

# Source: erasmus/utils/protocols/protocol_cli.py


logger = get_logger(__name__)


def add_protocol_management_commands(parser: argparse.ArgumentParser) -> None:
"""Add protocol management commands to the argument parser."""
protocol_group = parser.add_argument_group("Protocol Management Commands")

# Restore protocol command
protocol_group.add_argument(
"--restore-protocol",
type=str,
help="Restore a protocol by name from the protocol directory",
)

# Select protocol command
protocol_group.add_argument(
"--select-protocol",
action="store_true",
help="List available protocols and select one to load",
)

# Store protocol command
protocol_group.add_argument(
"--store-protocol", type=str, help="Store the current protocol in the protocol directory"
)

# Delete protocol command
protocol_group.add_argument(
"--delete-protocol", type=str, help="Delete a protocol from the protocol directory"
)


def update_context_with_protocol(protocol_name: str) -> None:
"""Update the context object in the IDE environment rules file with the protocol."""
rules_path = get_rules_path()
# Create the file if it doesn't exist
if not rules_path.exists():
rules_path.parent.mkdir(parent=True, exist_ok=True)
rules_path.write_text(json.dumps({"protocols": [protocol_name]}, indent=2))
logger.info(f"Created new rules file at {rules_path}")
return

# Read the existing rules
try:
rules = json.loads(rules_path.read_text())
except json.JSONDecodeError:
logger.error(f"Invalid JSON in rules file: {rules_path}")
rules = {"protocols": []}

# Update the protocols list
if "protocols" not in rules:
rules["protocols"] = []

# Add the protocol as the first entry if it's not already there
if protocol_name not in rules["protocols"]:
rules["protocols"].insert(0, protocol_name)

# Write the updated rules
rules_path.write_text(json.dumps(rules, indent=2))

logger.info(f"Updated rules file with protocol: {protocol_name}")


async def handle_protocol_management_commands(args: argparse.Namespace) -> None:
"""Handle protocol management commands."""
if not (
args.restore_protocol or args.select_protocol or args.store_protocol or args.delete_protocol
):
return

# Initialize the protocol manager
protocol_manager = await ProtocolManager.create()

# Create protocols directory if it doesn't exist
protocols_dir = protocol_manager.path_manager.stored_protocols_dir
protocols_dir.mkdir(parents=True, exist_ok=True)

# Handle restore protocol command
if args.restore_protocol:
protocol_name = args.restore_protocol
protocol_file = protocols_dir / f"{protocol_name}.json"

if not protocol_file.exists():
logger.error(f"Protocol file not found: {protocol_file}")
return

try:
protocol_data = json.loads(protocol_file.read_text())

# Update the context with the protocol
update_context_with_protocol(protocol_name)

logger.info(f"Restored protocol: {protocol_name}")
except Exception as e:
logger.error(f"Error restoring protocol {protocol_name}: {e}")

# Handle select protocol command
if args.select_protocol:
# List available protocols
available_protocols = []
for protocol in protocol_manager.protocols.values():
protocol_name = protocol.name
protocol_file = protocols_dir / f"{protocol_name}.json"
if protocol_file.exists():
available_protocols.append(protocol_name)

if not available_protocols:
logger.info("No protocols available to select")
return

# Display available protocols
print("\nAvailable Protocols:")
for i, protocol_name in enumerate(available_protocols):
print(f"{i + 1}. {protocol_name}")

# Get user input
try:
selection = int(input("\nSelect a protocol (number): "))
if selection < 1 or selection > len(available_protocols):
logger.error("Invalid selection")
return

selected_protocol = available_protocols[selection - 1]

# Update the context with the selected protocol
update_context_with_protocol(selected_protocol)

logger.info(f"Selected protocol: {selected_protocol}")
except ValueError:
logger.error("Invalid input")

# Handle store protocol command
if args.store_protocol:
protocol_name = args.store_protocol

# Find the protocol in the registry
protocol_data = None
for protocol in protocol_manager.protocols.values():
if protocol.name == protocol_name:
protocol_data = protocol
break

if not protocol_data:
logger.error(f"Protocol not found in registry: {protocol_name}")
return

# Store the protocol
protocol_file = protocols_dir / f"{protocol_name}.json"
protocol_file.write_text(json.dumps(protocol_data.model_dump(), indent=2))

logger.info(f"Stored protocol: {protocol_name}")

# Handle delete protocol command
if args.delete_protocol:
protocol_name = args.delete_protocol
protocol_file = protocols_dir / f"{protocol_name}.json"

if not protocol_file.exists():
logger.error(f"Protocol file not found: {protocol_file}")
return

try:
protocol_file.unlink()
logger.info(f"Deleted protocol: {protocol_name}")
except Exception as e:
logger.error(f"Error deleting protocol {protocol_name}: {e}")

# Source: erasmus/utils/protocols/cli.py
"""Protocol CLI commands."""



logger = logging.getLogger(__name__)


def get_path_manager(project_root: Optional[Path] = None) -> PathManager:
"""Get a PathManager instance.

Args:
project_root: Optional project root path. If not provided, uses current directory.

Returns:
PathManager: A PathManager instance
"""
return PathManager(project_root)


def handle_protocol_commands(args: Dict[str, Any]) -> Dict[str, Any]:
"""Handle protocol-related commands.

Args:
args: Command arguments

Returns:
Dict[str, Any]: Command results
"""
path_manager = get_path_manager()
path_manager.ensure_directories()

# Initialize protocol integration
protocol_manager = ProtocolManager()
protocol_server = ProtocolServer(setup_paths=path_manager)

# Register prompts
protocol_manager.register_default_prompts()

# Handle JSON context if provided
context = {}
if args.get("json_context"):
try:
context = json.loads(args["json_context"])
except json.JSONDecodeError as e:
logger.error(f"Failed to parse JSON context: {e}")
return {"error": f"Invalid JSON context: {e}"}

command = args.get("protocol_command")
if not command:
return {"error": "No protocol command specified"}

try:
if command == "list":
# List available protocols
protocols = protocol_manager.list_protocols()
return {"protocols": protocols}

elif command == "restore":
# Restore protocol from backup
protocol_name = args.get("protocol_name")
if not protocol_name:
return {"error": "Protocol name required for restore"}

result = protocol_manager.restore_protocol(protocol_name)
return {"result": result}

elif command == "select":
# Select a protocol
protocol_name = args.get("protocol_name")
if not protocol_name:
return {"error": "Protocol name required for selection"}

result = update_context_with_protocol(protocol_name, context)
return {"result": result}

elif command == "store":
# Store a protocol
protocol_name = args.get("protocol_name")
if not protocol_name:
return {"error": "Protocol name required for storage"}

result = protocol_manager.store_protocol(protocol_name)
return {"result": result}

elif command == "delete":
# Delete a protocol
protocol_name = args.get("protocol_name")
if not protocol_name:
return {"error": "Protocol name required for deletion"}

result = protocol_manager.delete_protocol(protocol_name)
return {"result": result}

elif command == "execute":
# Execute a protocol
protocol_name = args.get("protocol_name")
if not protocol_name:
return {"error": "Protocol name required for execution"}

artifacts = protocol_server.execute_protocol(protocol_name, context)
return {"artifacts": [artifact.model_dump() for artifact in artifacts]}

elif command == "workflow":
# Get protocol workflow
protocol_name = args.get("protocol_name")
if not protocol_name:
return {"error": "Protocol name required for workflow"}

workflow = protocol_server.get_protocol_workflow(protocol_name)
return {"workflow": workflow}

else:
return {"error": f"Unknown protocol command: {command}"}

except Exception as e:
logger.error(f"Error handling protocol command {command}: {e}")
return {"error": str(e)}


def update_context_with_protocol(protocol_name: str, context: Dict[str, Any]) -> Dict[str, Any]:
"""Update the context with the selected protocol.

Args:
protocol_name: Name of the protocol to select
context: Current context

Returns:
Dict[str, Any]: Updated context
"""
setup_paths = SetupPaths.with_project_root(Path.cwd())

# Create rules file if it doesn't exist
if not setup_paths.rules_file.exists():
setup_paths.rules_file.touch()

# Read existing rules
try:
with open(setup_paths.rules_file, "r") as f:
rules = json.load(f)
except (json.JSONDecodeError, FileNotFoundError):
rules = {}

# Update protocols list
if "protocols" not in rules:
rules["protocols"] = []

if protocol_name not in rules["protocols"]:
rules["protocols"].append(protocol_name)

# Load protocol markdown if available
protocol_file = setup_paths.protocols_dir / "stored" / f"{protocol_name}.md"
if protocol_file.exists():
with open(protocol_file, "r") as f:
protocol_content = f.read()
context["protocol_content"] = protocol_content

# Write updated rules
with open(setup_paths.rules_file, "w") as f:
json.dump(rules, f, indent=2)

return context


def add_protocol_commands(parser: argparse.ArgumentParser) -> None:
"""Add protocol-related commands to the argument parser."""
# Create a subparser for protocol commands
subparsers = parser.add_subparsers(dest="subcommand", help="Protocol management commands")

# Protocol list command
list_parser = subparsers.add_parser("list", help="List all available protocols")

# Protocol restore command
restore_parser = subparsers.add_parser("restore", help="Restore a protocol by name")
restore_parser.add_argument("name", type=str, help="Name of the protocol to restore")

# Protocol select command
select_parser = subparsers.add_parser(
"select", help="List available protocols and select one to load"
)

# Protocol store command
store_parser = subparsers.add_parser("store", help="Store a protocol in the protocol directory")
store_parser.add_argument("name", type=str, help="Name of the protocol to store")

# Protocol delete command
delete_parser = subparsers.add_parser(
"delete", help="Delete a protocol from the protocol directory"
)
delete_parser.add_argument("name", type=str, help="Name of the protocol to delete")

# Protocol execute command
execute_parser = subparsers.add_parser("execute", help="Execute a specific protocol")
execute_parser.add_argument("name", type=str, help="Name of the protocol to execute")
execute_parser.add_argument(
"--context", type=str, help="JSON string containing context for protocol execution"
)

# Protocol workflow command
workflow_parser = subparsers.add_parser(
"workflow", help="Run a workflow starting from a specific protocol"
)
workflow_parser.add_argument(
"name", type=str, help="Name of the protocol to start the workflow from"
)
workflow_parser.add_argument(
"--context", type=str, help="JSON string containing context for workflow execution"
)


def get_ide_env_rules_path() -> Path:
"""Get the path to the IDE environment rules file."""

# Use SetupPaths to get the correct rules file path based on IDE environment
setup_paths = SetupPaths.with_project_root(Path.cwd())
return setup_paths.rules_file


async def handle_protocol_commands(args: argparse.Namespace) -> None:
"""Handle protocol-related commands."""
if not args.subcommand:
return

# Create setup paths
setup_paths = SetupPaths.with_project_root(Path.cwd())

# Initialize the protocol integration
protocol_integration = ProtocolIntegration(setup_paths)
await protocol_integration.initialize()

# Register prompt functions
protocol_integration.register_protocol_prompts()

# Create protocols directory if it doesn't exist
protocols_dir = setup_paths.protocols_dir / "stored"
protocols_dir.mkdir(parents=True, exist_ok=True)

# Load the registry
registry_path = setup_paths.protocols_dir / "agent_registry.json"
if not registry_path.exists():
logger.error(f"Registry file not found: {registry_path}")
return

# Load the registry
with open(registry_path, "r") as f:
registry_data = json.load(f)

# Parse context if provided
context = {}
if hasattr(args, "context") and args.context:
try:
context = json.loads(args.context)
except json.JSONDecodeError:
logger.error("Invalid JSON in protocol context")
return

# Handle protocol list command
if args.subcommand == "list":
protocols = protocol_integration.list_protocols()
print("\nAvailable Protocols:")
for protocol in protocols:
print(f"- {protocol.name} ({protocol.role})")
print(f"  Triggers: {', '.join(protocol.triggers)}")
print(f"  Produces: {', '.join(protocol.produces)}")
print(f"  Consumes: {', '.join(protocol.consumes)}")
print()

# Handle protocol restore command
elif args.subcommand == "restore":
protocol_name = args.name
protocol_file = protocols_dir / f"{protocol_name}.json"

if not protocol_file.exists():
logger.error(f"Protocol file not found: {protocol_file}")
return

try:
with open(protocol_file, "r") as f:
protocol_data = json.load(f)

# Update the context with the protocol
update_context_with_protocol(protocol_name, context)

logger.info(f"Restored protocol: {protocol_name}")
except Exception as e:
logger.error(f"Error restoring protocol {protocol_name}: {e}")

# Handle protocol select command
elif args.subcommand == "select":
# List available protocols
available_protocols = []
for agent in registry_data["agents"]:
protocol_name = agent["name"]
protocol_file = protocols_dir / f"{protocol_name}.json"
if protocol_file.exists():
available_protocols.append(protocol_name)

if not available_protocols:
logger.info("No protocols available to select")
return

# Display available protocols
print("\nAvailable Protocols:")
for i, protocol_name in enumerate(available_protocols):
print(f"{i + 1}. {protocol_name}")

# Get user input
try:
selection = int(input("\nSelect a protocol (number): "))
if selection < 1 or selection > len(available_protocols):
logger.error("Invalid selection")
return

selected_protocol = available_protocols[selection - 1]

# Update the context with the selected protocol
update_context_with_protocol(selected_protocol, context)

logger.info(f"Selected protocol: {selected_protocol}")
except ValueError:
logger.error("Invalid input")

# Handle protocol store command
elif args.subcommand == "store":
protocol_name = args.name

# Find the protocol in the registry
protocol_data = None
for agent in registry_data["agents"]:
if agent["name"] == protocol_name:
protocol_data = agent
break

if not protocol_data:
logger.error(f"Protocol not found in registry: {protocol_name}")
return

# Store the protocol
protocol_file = protocols_dir / f"{protocol_name}.json"
with open(protocol_file, "w") as f:
json.dump(protocol_data, f, indent=2)

logger.info(f"Stored protocol: {protocol_name}")

# Handle protocol delete command
elif args.subcommand == "delete":
protocol_name = args.name
protocol_file = protocols_dir / f"{protocol_name}.json"

if not protocol_file.exists():
logger.error(f"Protocol file not found: {protocol_file}")
return

try:
os.remove(protocol_file)
logger.info(f"Deleted protocol: {protocol_name}")
except Exception as e:
logger.error(f"Error deleting protocol {protocol_name}: {e}")

# Handle protocol execute command
elif args.subcommand == "execute":
try:
result = await protocol_integration.execute_protocol(args.name, context)
print(f"\nProtocol Execution Result:")
print(f"Protocol: {args.name}")
print(f"Artifacts: {len(result['artifacts'])}")
print(f"Next Transitions: {len(result['next_transitions'])}")

# Print artifacts
print("\nArtifacts:")
for artifact in result["artifacts"]:
print(f"- {artifact['name']} ({artifact['type']})")

# Print transitions
print("\nNext Transitions:")
for transition in result["next_transitions"]:
print(
f"- {transition['from_agent']} -> {transition['to_agent']} ({transition['trigger']})"
)

except Exception as e:
logger.error(f"Error executing protocol {args.name}: {e}")

# Handle protocol workflow command
elif args.subcommand == "workflow":
try:
result = await protocol_integration.run_workflow(args.name, context)
print(f"\nWorkflow Execution Result:")
print(f"Starting Protocol: {args.name}")
print(f"Total Steps: {len(result['workflow_results'])}")

# Print workflow steps
print("\nWorkflow Steps:")
for i, step in enumerate(result["workflow_results"]):
print(f"{i + 1}. {step['protocol']}")
print(f"   Artifacts: {len(step['result']['artifacts'])}")
print(f"   Next Transitions: {len(step['result']['next_transitions'])}")

except Exception as e:
logger.error(f"Error running workflow from {args.name}: {e}")

# Source: erasmus/utils/protocols/__init__.py
"""Protocol system for Erasmus.

This package provides a protocol system for managing agent workflows within the Erasmus framework.
"""


__all__ = [
"Protocol",
"ProtocolArtifact",
"ProtocolTransition",
"ProtocolManager",
"ProtocolRegistry",
"ProtocolServer",
"ProtocolExecutionRequest",
"ProtocolExecutionResponse",
"ProtocolIntegration",
"add_protocol_commands",
"handle_protocol_commands",
"update_context_with_protocol",
]

# Source: erasmus/utils/protocols/example.py


logger = get_logger(__name__)


# Example prompt functions for different protocols
def product_owner_prompt(context: dict) -> dict:
"""Example prompt function for the Product Owner Agent."""
# In a real implementation, this would use your AI model or other logic
return {
".erasmus/.architecture.md": "# Project Architecture\n\nThis is a sample architecture document.",
".progress.md": "# Development Progress\n\nCurrent progress: 0%",
}


def developer_prompt(context: dict) -> dict:
"""Example prompt function for the Developer Agent."""
# In a real implementation, this would use your AI model or other logic
return {
".tasks.md": "# Development Tasks\n\n1. Implement feature X\n2. Write tests\n3. Update documentation"
}


async def run_protocol_example():
"""Run an example of the protocol system."""
# Create setup paths
setup_paths = SetupPaths.with_project_root(Path.cwd())

# Initialize the protocol integration
protocol_integration = ProtocolIntegration(setup_paths)
await protocol_integration.initialize()

# Register prompt functions
protocol_integration.register_protocol_prompts()

# Example: Execute a single protocol
context = {
"project_name": "Example Project",
"description": "A sample project to demonstrate the protocol system",
}

try:
# Execute the Product Owner Agent protocol
result = await protocol_integration.execute_protocol("Product Owner Agent", context)
logger.info(f"Product Owner Agent result: {result}")

# Example: Run a workflow
workflow_result = await protocol_integration.run_workflow("Product Owner Agent", context)
logger.info(f"Workflow result: {workflow_result}")

except Exception as e:
logger.error(f"Error running protocol example: {e}")


if __name__ == "__main__":
asyncio.run(run_protocol_example())